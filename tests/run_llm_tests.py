#!/usr/bin/env python3
"""
AgenticX LLM Module Test Runner

å¿«é€Ÿè¿è¡Œ agenticx.llms æ¨¡å—çš„æ‰€æœ‰æµ‹è¯•ã€‚
"""

import sys
import os
import traceback
from unittest.mock import patch, MagicMock
import asyncio
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

def run_basic_tests():
    """è¿è¡ŒåŸºç¡€åŠŸèƒ½æµ‹è¯•"""
    print("=== AgenticX LLM Module Test Runner ===\n")
    
    try:
        # æµ‹è¯•å¯¼å…¥
        print("1. æµ‹è¯•æ¨¡å—å¯¼å…¥...")
        from agenticx.llms import (
            BaseLLMProvider,
            LLMResponse,
            TokenUsage,
            LLMChoice,
            LiteLLMProvider,
            OpenAIProvider,
            AnthropicProvider,
            OllamaProvider,
            GeminiProvider
        )
        print("   âœ… æ‰€æœ‰LLMç±»å¯¼å…¥æˆåŠŸ\n")
        
        # æµ‹è¯•æ•°æ®ç±»
        print("2. æµ‹è¯•æ•°æ®ç±»...")
        usage = TokenUsage(prompt_tokens=10, completion_tokens=20, total_tokens=30)
        choice = LLMChoice(index=0, content="Test content", finish_reason="stop")
        response = LLMResponse(
            id="test-123",
            model_name="test-model",
            created=int(time.time()),
            content="Test content",
            choices=[choice],
            token_usage=usage,
            cost=0.01
        )
        
        assert usage.total_tokens == 30
        assert choice.content == "Test content"
        assert response.content == "Test content"
        assert len(response.choices) == 1
        print("   âœ… æ•°æ®ç±»åˆ›å»ºå’Œå±æ€§æµ‹è¯•é€šè¿‡\n")
        
        # æµ‹è¯•ä¾¿åˆ©æä¾›å•†ç±»
        print("3. æµ‹è¯•ä¾¿åˆ©æä¾›å•†ç±»...")
        openai_provider = OpenAIProvider(model="gpt-4")
        anthropic_provider = AnthropicProvider(model="claude-3-opus-20240229")
        ollama_provider = OllamaProvider(model="ollama/llama3")
        gemini_provider = GeminiProvider(model="gemini/gemini-pro")
        
        assert isinstance(openai_provider, LiteLLMProvider)
        assert isinstance(anthropic_provider, LiteLLMProvider)
        assert isinstance(ollama_provider, LiteLLMProvider)
        assert isinstance(gemini_provider, LiteLLMProvider)
        
        assert openai_provider.model == "gpt-4"
        assert anthropic_provider.model == "claude-3-opus-20240229"
        print("   âœ… ä¾¿åˆ©æä¾›å•†ç±»æµ‹è¯•é€šè¿‡\n")
        
        # æµ‹è¯•LiteLLMProviderï¼ˆæ¨¡æ‹Ÿè°ƒç”¨ï¼‰
        print("4. æµ‹è¯•LiteLLMProviderï¼ˆæ¨¡æ‹Ÿè°ƒç”¨ï¼‰...")
        
        # åˆ›å»ºæ¨¡æ‹Ÿå“åº”
        mock_response = MagicMock()
        mock_response.id = "chatcmpl-test"
        mock_response.model = "gpt-3.5-turbo"
        mock_response.created = int(time.time())
        
        # æ¨¡æ‹Ÿchoices
        mock_choice = MagicMock()
        mock_choice.index = 0
        mock_choice.finish_reason = "stop"
        mock_choice.message.content = "Hello from test!"
        mock_response.choices = [mock_choice]
        
        # æ¨¡æ‹Ÿusage
        mock_response.usage = {
            "prompt_tokens": 5,
            "completion_tokens": 10,
            "total_tokens": 15
        }
        
        # æ¨¡æ‹Ÿcost
        mock_response.cost = {"completion_cost": 0.0001}
        mock_response._response_ms = 200
        mock_response.custom_llm_provider = "openai"
        
        # æµ‹è¯•åŒæ­¥è°ƒç”¨
        with patch('litellm.completion', return_value=mock_response):
            provider = LiteLLMProvider(model="gpt-3.5-turbo")
            result = provider.invoke("Hello, world!")
            
            assert isinstance(result, LLMResponse)
            assert result.content == "Hello from test!"
            assert result.token_usage.total_tokens == 15
            assert result.cost == 0.0001
        
        print("   âœ… LiteLLMProvideråŒæ­¥è°ƒç”¨æµ‹è¯•é€šè¿‡\n")
        
        # æµ‹è¯•æµå¼è°ƒç”¨ï¼ˆæ¨¡æ‹Ÿï¼‰
        print("5. æµ‹è¯•æµå¼è°ƒç”¨ï¼ˆæ¨¡æ‹Ÿï¼‰...")
        
        # åˆ›å»ºæ¨¡æ‹Ÿæµå¼å“åº”
        def create_mock_chunk(content):
            chunk = MagicMock()
            chunk.choices = [MagicMock()]
            chunk.choices[0].delta.content = content
            return chunk
        
        mock_chunks = [
            create_mock_chunk("Hello "),
            create_mock_chunk("from "),
            create_mock_chunk("stream!"),
            create_mock_chunk(None)  # ç©ºå†…å®¹å—
        ]
        
        with patch('litellm.completion', return_value=mock_chunks):
            provider = LiteLLMProvider(model="gpt-3.5-turbo")
            stream_result = "".join([chunk for chunk in provider.stream("Stream test")])
            
            assert stream_result == "Hello from stream!"
        
        print("   âœ… æµå¼è°ƒç”¨æµ‹è¯•é€šè¿‡\n")
        
        print("ğŸ‰ æ‰€æœ‰LLMæ¨¡å—æµ‹è¯•éƒ½é€šè¿‡äº†ï¼AgenticX LLM æ¨¡å—åŠŸèƒ½æ­£å¸¸ã€‚")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
        return False

def run_async_tests():
    """è¿è¡Œå¼‚æ­¥åŠŸèƒ½æµ‹è¯•"""
    print("\n=== å¼‚æ­¥åŠŸèƒ½æµ‹è¯• ===")
    
    try:
        from agenticx.llms import LiteLLMProvider, LLMResponse
        from unittest.mock import AsyncMock
        
        async def test_async_invoke():
            # åˆ›å»ºæ¨¡æ‹Ÿå¼‚æ­¥å“åº”
            mock_response = MagicMock()
            mock_response.id = "async-test"
            mock_response.model = "gpt-4"
            mock_response.created = int(time.time())
            
            mock_choice = MagicMock()
            mock_choice.index = 0
            mock_choice.finish_reason = "stop"
            mock_choice.message.content = "Async response!"
            mock_response.choices = [mock_choice]
            
            mock_response.usage = {
                "prompt_tokens": 8,
                "completion_tokens": 12,
                "total_tokens": 20
            }
            mock_response.cost = {"completion_cost": 0.0002}
            
            with patch('litellm.acompletion', new_callable=AsyncMock, return_value=mock_response):
                provider = LiteLLMProvider(model="gpt-4")
                result = await provider.ainvoke("Async test")
                
                assert isinstance(result, LLMResponse)
                assert result.content == "Async response!"
                assert result.token_usage.total_tokens == 20
                
            return True
        
        async def test_async_stream():
            # åˆ›å»ºæ¨¡æ‹Ÿå¼‚æ­¥æµ
            async def mock_async_stream():
                chunks = ["Async ", "stream ", "test!"]
                for chunk_content in chunks:
                    chunk = MagicMock()
                    chunk.choices = [MagicMock()]
                    chunk.choices[0].delta.content = chunk_content
                    yield chunk
            
            with patch('litellm.acompletion', new_callable=AsyncMock, return_value=mock_async_stream()):
                provider = LiteLLMProvider(model="gpt-4")
                stream_result = "".join([chunk async for chunk in provider.astream("Async stream test")])
                
                assert stream_result == "Async stream test!"
                
            return True
        
        # è¿è¡Œå¼‚æ­¥æµ‹è¯•
        result1 = asyncio.run(test_async_invoke())
        result2 = asyncio.run(test_async_stream())
        
        if result1 and result2:
            print("   âœ… å¼‚æ­¥è°ƒç”¨æµ‹è¯•é€šè¿‡")
            print("   âœ… å¼‚æ­¥æµå¼è°ƒç”¨æµ‹è¯•é€šè¿‡")
            return True
        else:
            return False
            
    except Exception as e:
        print(f"âŒ å¼‚æ­¥æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

if __name__ == "__main__":
    success = run_basic_tests()
    
    if success:
        success_async = run_async_tests()
        if success_async:
            print("\nğŸŠ æ‰€æœ‰æµ‹è¯•ï¼ˆåŒ…æ‹¬å¼‚æ­¥åŠŸèƒ½ï¼‰éƒ½é€šè¿‡äº†ï¼")
        else:
            print("\nâš ï¸ åŸºç¡€æµ‹è¯•é€šè¿‡ï¼Œä½†å¼‚æ­¥åŠŸèƒ½æµ‹è¯•å¤±è´¥")
    
    sys.exit(0 if success else 1) 