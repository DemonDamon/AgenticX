"""SPO (Subject-Predicate-Object) Extractor for Knowledge Graph Construction

This module provides a unified SPO extraction approach with custom schema support,
extracting entities, relationships, and attributes in a single LLM call.
"""

import json
import os
from typing import Any, Dict, List, Optional, Tuple
from loguru import logger

from .models import Entity, EntityType, Relationship, RelationType


class SPOExtractor:
    """Unified SPO extractor with custom schema and prompt template support"""
    
    def __init__(self, llm_client=None, prompt_manager=None, custom_schema: Optional[Dict[str, Any]] = None, config: Optional[Dict[str, Any]] = None):
        self.llm_client = llm_client
        self.prompt_manager = prompt_manager
        self.config = config or {}
        
        # Use custom schema if provided, otherwise use default
        if custom_schema:
            self.schema = custom_schema
            logger.info("ğŸ¯ ä½¿ç”¨å®šåˆ¶Schema")
        else:
            # Default schema
            self.schema = {
                "Nodes": ["person", "organization", "location", "event", "concept", "technology", "product"],
                "Relations": ["related_to", "part_of", "located_in", "works_for", "created_by", "influences", "depends_on"],
                "Attributes": ["name", "description", "type", "status", "date", "profession", "title"]
            }
            logger.info("ğŸ“‹ ä½¿ç”¨é»˜è®¤Schema")
        
        # æå–é¢†åŸŸä¿¡æ¯
        self.domain_info = self.schema.get('domain_info', {})
        self.primary_domain = self.domain_info.get('primary_domain', 'é€šç”¨')
        self.key_concepts = ', '.join(self.domain_info.get('key_concepts', []))
        
        logger.info(f"ğŸ”§ SPOæŠ½å–å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.debug(f"ğŸ“‹ Schema: {len(self.schema['Nodes'])} å®ä½“ç±»å‹, {len(self.schema['Relations'])} å…³ç³»ç±»å‹, {len(self.schema['Attributes'])} å±æ€§ç±»å‹")
        logger.debug(f"ğŸ¯ ä¸»è¦é¢†åŸŸ: {self.primary_domain}")
    
    def extract(self, text: str, **kwargs) -> Tuple[List[Entity], List[Relationship]]:
        """Extract entities and relationships in a single call
        
        Args:
            text: Text to extract from
            **kwargs: Additional parameters
            
        Returns:
            Tuple of (entities, relationships)
        """
        logger.info(f"ğŸ” å¼€å§‹SPOæŠ½å–ï¼Œæ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        
        if not self.llm_client:
            raise ValueError("LLM client is required for SPO extraction")
        
        try:
            # Build prompt
            logger.debug("ğŸ“ æ„å»ºSPOæŠ½å–æç¤ºè¯...")
            prompt = self._build_spo_prompt(text)
            
            # Call LLM
            logger.debug("ğŸ¤– è°ƒç”¨LLMè¿›è¡ŒSPOæŠ½å–")
            response = self.llm_client.call(prompt)
            logger.debug(f"ğŸ“„ LLMå“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
            
            # Parse response
            logger.debug("ğŸ” è§£æLLMå“åº”...")
            spo_data = self._parse_spo_response(response)
            logger.debug(f"ğŸ“Š è§£æç»“æœ: {len(spo_data.get('entity_types', {}))} ä¸ªå®ä½“ç±»å‹, {len(spo_data.get('triples', []))} ä¸ªä¸‰å…ƒç»„")
            
            # Convert to entities and relationships
            logger.debug("ğŸ”„ è½¬æ¢ä¸ºå®ä½“å’Œå…³ç³»å¯¹è±¡...")
            entities, relationships = self._convert_spo_to_objects(spo_data, text, **kwargs)
            
            logger.success(f"âœ… SPOæŠ½å–å®Œæˆ: {len(entities)} ä¸ªå®ä½“, {len(relationships)} ä¸ªå…³ç³»")
            
            return entities, relationships
            
        except Exception as e:
            logger.error(f"âŒ SPOæŠ½å–å¤±è´¥: {e}")
            logger.debug(f"âŒ é”™è¯¯è¯¦æƒ…: {type(e).__name__}: {str(e)}")
            import traceback
            logger.debug(f"âŒ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return [], []
    
    def _find_entity_id(self, entity_name: str, entity_id_map: Dict[str, str]) -> Optional[str]:
        """æŸ¥æ‰¾å®ä½“IDï¼Œæ”¯æŒæ™ºèƒ½æ¨¡ç³ŠåŒ¹é…"""
        # 1. ç²¾ç¡®åŒ¹é…
        if entity_name in entity_id_map:
            return entity_id_map[entity_name]
        
        # 2. æ ‡å‡†åŒ–åç§°åŒ¹é…
        normalized_target = self._normalize_entity_name(entity_name)
        for name, entity_id in entity_id_map.items():
            if self._normalize_entity_name(name) == normalized_target:
                logger.debug(f"ğŸ” æ ‡å‡†åŒ–åŒ¹é…æˆåŠŸ: '{entity_name}' -> '{name}'")
                return entity_id
        
        # 3. ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆå¤„ç†å¤åˆè¯ã€ç¼©å†™ç­‰ï¼‰
        best_match = None
        best_score = 0.0
        
        for name, entity_id in entity_id_map.items():
            score = self._calculate_similarity(entity_name, name)
            if score > best_score and score >= 0.8:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                best_score = score
                best_match = (name, entity_id)
        
        if best_match:
            logger.debug(f"ğŸ” ç›¸ä¼¼åº¦åŒ¹é…æˆåŠŸ: '{entity_name}' -> '{best_match[0]}' (ç›¸ä¼¼åº¦: {best_score:.2f})")
            return best_match[1]
        
        # 4. åŒ…å«å…³ç³»åŒ¹é…ï¼ˆé™ä½ä¼˜å…ˆçº§ï¼‰
        for name, entity_id in entity_id_map.items():
            if len(normalized_target) > 3:  # é¿å…çŸ­è¯è¯¯åŒ¹é…
                if normalized_target in self._normalize_entity_name(name) or self._normalize_entity_name(name) in normalized_target:
                    logger.debug(f"ğŸ” åŒ…å«åŒ¹é…æˆåŠŸ: '{entity_name}' -> '{name}'")
                    return entity_id
        
        return None
    
    def _normalize_entity_name(self, name: str) -> str:
        """æ ‡å‡†åŒ–å®ä½“åç§°"""
        import re
        # è½¬æ¢ä¸ºå°å†™
        normalized = name.lower().strip()
        # æ›¿æ¢è¿å­—ç¬¦å’Œä¸‹åˆ’çº¿ä¸ºç©ºæ ¼
        normalized = re.sub(r'[-_]', ' ', normalized)
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼ˆä¿ç•™å­—æ¯æ•°å­—å’Œç©ºæ ¼ï¼‰
        normalized = re.sub(r'[^\w\s]', '', normalized)
        # åˆå¹¶å¤šä¸ªç©ºæ ¼ä¸ºå•ä¸ªç©ºæ ¼
        normalized = re.sub(r'\s+', ' ', normalized)
        return normalized.strip()
    
    def _select_template(self, text: str) -> str:
        """æ™ºèƒ½é€‰æ‹©SPOæŠ½å–æ¨¡æ¿"""
        text_length = len(text)
        
        # 1. æ ¹æ®æ–‡æœ¬é•¿åº¦é€‰æ‹©
        if text_length < 500:
            logger.debug(f"ğŸ“ æ–‡æœ¬è¾ƒçŸ­({text_length}å­—ç¬¦)ï¼Œé€‰æ‹©ç®€åŒ–æ¨¡æ¿")
            return "simple_template"
        
        # 2. æ ¹æ®é¢†åŸŸä¿¡æ¯é€‰æ‹©é¢†åŸŸç‰¹å®šæ¨¡æ¿
        if hasattr(self, 'primary_domain') and self.primary_domain:
            domain_lower = self.primary_domain.lower()
            
            # æŠ€æœ¯é¢†åŸŸ
            if any(keyword in domain_lower for keyword in ['æŠ€æœ¯', 'ç§‘æŠ€', 'äººå·¥æ™ºèƒ½', 'ai', 'technology', 'tech']):
                logger.debug(f"ğŸ”§ æ£€æµ‹åˆ°æŠ€æœ¯é¢†åŸŸ: {self.primary_domain}")
                return "domain_templates.technology"
            
            # å•†ä¸šé¢†åŸŸ
            elif any(keyword in domain_lower for keyword in ['å•†ä¸š', 'ä¸šåŠ¡', 'ç®¡ç†', 'business', 'management']):
                logger.debug(f"ğŸ’¼ æ£€æµ‹åˆ°å•†ä¸šé¢†åŸŸ: {self.primary_domain}")
                return "domain_templates.business"
            
            # å­¦æœ¯é¢†åŸŸ
            elif any(keyword in domain_lower for keyword in ['å­¦æœ¯', 'ç ”ç©¶', 'ç§‘å­¦', 'academic', 'research', 'science']):
                logger.debug(f"ğŸ“ æ£€æµ‹åˆ°å­¦æœ¯é¢†åŸŸ: {self.primary_domain}")
                return "domain_templates.academic"
        
        # 3. æ ¹æ®æ–‡æœ¬å†…å®¹ç‰¹å¾é€‰æ‹©
        text_lower = text.lower()
        
        # æŠ€æœ¯æ–‡æ¡£ç‰¹å¾
        tech_keywords = ['ç®—æ³•', 'æ¨¡å‹', 'æ¡†æ¶', 'ç³»ç»Ÿ', 'ä»£ç ', 'algorithm', 'model', 'framework', 'system']
        if any(keyword in text_lower for keyword in tech_keywords):
            logger.debug("ğŸ”§ æ ¹æ®å†…å®¹ç‰¹å¾é€‰æ‹©æŠ€æœ¯æ¨¡æ¿")
            return "domain_templates.technology"
        
        # å•†ä¸šæ–‡æ¡£ç‰¹å¾
        business_keywords = ['å…¬å¸', 'å¸‚åœº', 'é”€å”®', 'å®¢æˆ·', 'ä¸šç»©', 'company', 'market', 'sales', 'customer']
        if any(keyword in text_lower for keyword in business_keywords):
            logger.debug("ğŸ’¼ æ ¹æ®å†…å®¹ç‰¹å¾é€‰æ‹©å•†ä¸šæ¨¡æ¿")
            return "domain_templates.business"
        
        # å­¦æœ¯æ–‡æ¡£ç‰¹å¾
        academic_keywords = ['è®ºæ–‡', 'ç ”ç©¶', 'å®éªŒ', 'ç†è®º', 'paper', 'research', 'experiment', 'theory']
        if any(keyword in text_lower for keyword in academic_keywords):
            logger.debug("ğŸ“ æ ¹æ®å†…å®¹ç‰¹å¾é€‰æ‹©å­¦æœ¯æ¨¡æ¿")
            return "domain_templates.academic"
        
        # 4. é»˜è®¤ä½¿ç”¨ä¸»æ¨¡æ¿
        logger.debug("ğŸ“„ ä½¿ç”¨é»˜è®¤ä¸»æ¨¡æ¿")
        return "template"
    
    def _calculate_similarity(self, name1: str, name2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªå®ä½“åç§°çš„ç›¸ä¼¼åº¦"""
        # æ ‡å‡†åŒ–åç§°
        norm1 = self._normalize_entity_name(name1)
        norm2 = self._normalize_entity_name(name2)
        
        # å¦‚æœå®Œå…¨ç›¸åŒ
        if norm1 == norm2:
            return 1.0
        
        # åˆ†è¯å¤„ç†
        words1 = set(norm1.split())
        words2 = set(norm2.split())
        
        # å¦‚æœå…¶ä¸­ä¸€ä¸ªæ˜¯å¦ä¸€ä¸ªçš„å­é›†
        if words1.issubset(words2) or words2.issubset(words1):
            return 0.9
        
        # è®¡ç®—Jaccardç›¸ä¼¼åº¦
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        if union == 0:
            return 0.0
        
        jaccard_score = intersection / union
        
        # å¤„ç†ç¼©å†™æƒ…å†µï¼ˆå¦‚ LLMs vs Large Language Modelsï¼‰
        if self._is_abbreviation_match(norm1, norm2):
            jaccard_score = max(jaccard_score, 0.85)
        
        # å¤„ç†ç¼–è¾‘è·ç¦»
        edit_distance_score = self._calculate_edit_distance_similarity(norm1, norm2)
        
        # ç»¼åˆè¯„åˆ†
        final_score = max(jaccard_score, edit_distance_score * 0.8)
        
        return final_score
    
    def _is_abbreviation_match(self, name1: str, name2: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºç¼©å†™åŒ¹é…"""
        words1 = name1.split()
        words2 = name2.split()
        
        # æ£€æŸ¥ä¸€ä¸ªæ˜¯å¦ä¸ºå¦ä¸€ä¸ªçš„é¦–å­—æ¯ç¼©å†™
        if len(words1) == 1 and len(words2) > 1:
            abbrev = ''.join([w[0] for w in words2 if w])
            return words1[0].replace('s', '') == abbrev.lower()  # å¤„ç†å¤æ•°å½¢å¼
        elif len(words2) == 1 and len(words1) > 1:
            abbrev = ''.join([w[0] for w in words1 if w])
            return words2[0].replace('s', '') == abbrev.lower()
        
        return False
    
    def _calculate_edit_distance_similarity(self, s1: str, s2: str) -> float:
        """è®¡ç®—ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦"""
        if len(s1) == 0 or len(s2) == 0:
            return 0.0
        
        # ç®€åŒ–çš„ç¼–è¾‘è·ç¦»è®¡ç®—
        max_len = max(len(s1), len(s2))
        if max_len == 0:
            return 1.0
        
        # è®¡ç®—å…¬å…±å­åºåˆ—é•¿åº¦
        common_chars = 0
        for char in set(s1):
            common_chars += min(s1.count(char), s2.count(char))
        
        return common_chars / max_len
    
    def _create_missing_entity(self, entity_name: str, entities: List, entity_id_map: Dict[str, str]) -> Optional[str]:
        """åŠ¨æ€åˆ›å»ºç¼ºå¤±çš„å®ä½“"""
        import uuid
        from .models import Entity, EntityType
        
        # è¿‡æ»¤æ‰è¿‡çŸ­æˆ–æ— æ„ä¹‰çš„å®ä½“åç§°
        if len(entity_name.strip()) < 2:
            return None
        
        # è¿‡æ»¤æ‰å¸¸è§çš„æ— æ„ä¹‰è¯æ±‡
        meaningless_words = {
            'information', 'data', 'system', 'method', 'approach', 'way', 'means',
            'process', 'technique', 'strategy', 'solution', 'result', 'output'
        }
        
        normalized_name = self._normalize_entity_name(entity_name)
        if normalized_name in meaningless_words:
            return None
        
        # ç”Ÿæˆæ–°çš„å®ä½“ID
        entity_id = str(uuid.uuid4())
        
        # æ¨æ–­å®ä½“ç±»å‹ï¼ˆç®€å•çš„å¯å‘å¼è§„åˆ™ï¼‰
        try:
            entity_type = self._infer_entity_type(entity_name)
            logger.debug(f"ğŸ” æ¨æ–­å®ä½“ç±»å‹: {entity_name} -> {entity_type.value}")
        except Exception as e:
            logger.warning(f"âš ï¸ å®ä½“ç±»å‹æ¨æ–­å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤ç±»å‹")
            from .models import EntityType
            entity_type = EntityType.CONCEPT
        
        # åˆ›å»ºæ–°å®ä½“
        try:
            new_entity = Entity(
                id=entity_id,
                name=entity_name,
                entity_type=entity_type,
                description=f"åŠ¨æ€åˆ›å»ºçš„å®ä½“: {entity_name}",
                confidence=0.7  # åŠ¨æ€åˆ›å»ºçš„å®ä½“ç½®ä¿¡åº¦è¾ƒä½
            )
            logger.debug(f"âœ… æˆåŠŸåˆ›å»ºå®ä½“: {entity_name} ({entity_type.value})")
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºå®ä½“å¤±è´¥: {e}")
            return None
        
        # æ·»åŠ åˆ°å®ä½“åˆ—è¡¨å’Œæ˜ å°„
        entities.append(new_entity)
        entity_id_map[entity_name] = entity_id
        
        return entity_id
    
    def _infer_entity_type(self, entity_name: str) -> 'EntityType':
        """æ¨æ–­å®ä½“ç±»å‹"""
        from .models import EntityType
        
        name_lower = entity_name.lower()
        
        # äººå‘˜ç›¸å…³
        if any(word in name_lower for word in ['äºº', 'è€…', 'å‘˜', 'person', 'researcher', 'author', 'developer']):
            return EntityType.PERSON
        
        # ç»„ç»‡ç›¸å…³
        if any(word in name_lower for word in ['å…¬å¸', 'ç»„ç»‡', 'æœºæ„', 'company', 'organization', 'institution']):
            return EntityType.ORGANIZATION
        
        # åœ°ç‚¹ç›¸å…³
        if any(word in name_lower for word in ['åœ°', 'å¸‚', 'å›½', 'location', 'city', 'country', 'place']):
            return EntityType.LOCATION
        
        # äº‹ä»¶ç›¸å…³
        if any(word in name_lower for word in ['è¿‡ç¨‹', 'æµç¨‹', 'æ“ä½œ', 'ä»»åŠ¡', 'process', 'procedure', 'operation', 'task', 'event']):
            return EntityType.EVENT
        
        # å¯¹è±¡ç›¸å…³ï¼ˆæŠ€æœ¯äº§å“ã€å·¥å…·ç­‰ï¼‰
        if any(word in name_lower for word in ['ç³»ç»Ÿ', 'å¹³å°', 'å·¥å…·', 'è½¯ä»¶', 'æ¨¡å‹', 'system', 'platform', 'tool', 'software', 'model']):
            return EntityType.OBJECT
        
        # æ—¶é—´ç›¸å…³
        if any(word in name_lower for word in ['æ—¶é—´', 'æ—¥æœŸ', 'å¹´', 'æœˆ', 'time', 'date', 'year', 'month']):
            return EntityType.TIME
        
        # æ¦‚å¿µç›¸å…³ï¼ˆç®—æ³•ã€æ–¹æ³•ã€ç†è®ºç­‰ï¼‰
        if any(word in name_lower for word in ['ç®—æ³•', 'æ–¹æ³•', 'æŠ€æœ¯', 'ç†è®º', 'æ¦‚å¿µ', 'algorithm', 'method', 'technique', 'theory', 'concept', 'approach']):
            return EntityType.CONCEPT
        
        # é»˜è®¤ä¸ºæ¦‚å¿µ
        return EntityType.CONCEPT
    
    def _build_spo_prompt(self, text: str) -> str:
        """Build SPO extraction prompt using prompt manager and custom schema"""
        
        if self.prompt_manager:
            # ä½¿ç”¨æç¤ºè¯ç®¡ç†å™¨ï¼Œæ™ºèƒ½é€‰æ‹©æ¨¡æ¿
            try:
                custom_schema_str = json.dumps(self.schema, ensure_ascii=False, indent=2)
                
                # æ™ºèƒ½é€‰æ‹©æ¨¡æ¿
                template_name = self._select_template(text)
                logger.info(f"ğŸ¯ é€‰æ‹©æ¨¡æ¿: {template_name}")
                
                # å¤„ç†é¢†åŸŸæ¨¡æ¿è·¯å¾„
                if template_name.startswith("domain_templates."):
                    domain_type = template_name.split(".")[-1]
                    prompt = self.prompt_manager.format_prompt(
                        "spo_extraction",
                        template_key=f"domain_templates.{domain_type}.template",
                        custom_schema=custom_schema_str,
                        primary_domain=self.primary_domain,
                        key_concepts=self.key_concepts,
                        text=text
                    )
                else:
                    prompt = self.prompt_manager.format_prompt(
                        "spo_extraction",
                        template_key=template_name,
                        custom_schema=custom_schema_str,
                        primary_domain=self.primary_domain,
                        key_concepts=self.key_concepts,
                        text=text
                    )
                
                if prompt:
                    logger.debug(f"ğŸ“„ ä½¿ç”¨{template_name}æ¨¡æ¿ç”ŸæˆSPOæŠ½å–æç¤ºè¯")
                    return prompt
                else:
                    logger.warning("âš ï¸ æç¤ºè¯æ¨¡æ¿åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯")
                    
            except Exception as e:
                logger.error(f"âŒ æç¤ºè¯æ¨¡æ¿å¤„ç†å¤±è´¥: {e}")
                logger.warning("ğŸ”„ å›é€€åˆ°é»˜è®¤æç¤ºè¯")
        
        # å›é€€åˆ°é»˜è®¤æç¤ºè¯
        schema_str = json.dumps(self.schema, ensure_ascii=False, indent=2)
        
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„çŸ¥è¯†å›¾è°±æ„å»ºä¸“å®¶ã€‚è¯·åŸºäºå®šåˆ¶Schemaåˆ†ææ–‡æœ¬ï¼ŒæŠ½å–å°½å¯èƒ½å¤šçš„æœ‰ä»·å€¼å®ä½“ã€å±æ€§å’Œå…³ç³»ï¼Œä»¥ç»“æ„åŒ–JSONæ ¼å¼è¿”å›ã€‚

å®šåˆ¶Schemaï¼š
```json
{schema_str}
```

é¢†åŸŸä¿¡æ¯ï¼š
- ä¸»è¦é¢†åŸŸï¼š{self.primary_domain}
- æ ¸å¿ƒæ¦‚å¿µï¼š{self.key_concepts}

æ–‡æœ¬å†…å®¹ï¼š
```
{text}
```

æŠ½å–æŒ‡å¯¼ï¼š
1. **ä¼˜å…ˆä½¿ç”¨å®šåˆ¶Schema**ï¼šä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°Schemaä¸­çš„ç±»å‹è¿›è¡ŒæŠ½å–
2. **å®Œæ•´æ€§**ï¼šä¸é—æ¼æ–‡æœ¬ä¸­çš„é‡è¦ä¿¡æ¯
3. **å‡†ç¡®æ€§**ï¼šç¡®ä¿æŠ½å–çš„å®ä½“å’Œå…³ç³»å‡†ç¡®æ— è¯¯
4. **ç®€æ´æ€§**ï¼šé¿å…å†—ä½™å’Œé‡å¤ä¿¡æ¯
5. **ä¸€è‡´æ€§**ï¼šå®ä½“åç§°åœ¨æ•´ä¸ªæŠ½å–è¿‡ç¨‹ä¸­ä¿æŒä¸€è‡´

è¾“å‡ºæ ¼å¼ï¼š
```json
{{
  "attributes": {{
    "å®ä½“åç§°": ["å±æ€§1: å€¼1", "å±æ€§2: å€¼2"]
  }},
  "triples": [
    ["å®ä½“1", "å…³ç³»", "å®ä½“2"]
  ],
  "entity_types": {{
    "å®ä½“åç§°": "å®ä½“ç±»å‹"
  }}
}}
```

åªè¿”å›JSONï¼Œæ— å…¶ä»–å†…å®¹ã€‚"""
        
        return prompt.strip()
    
    def _parse_spo_response(self, response: str) -> Dict[str, Any]:
        """Parse LLM response into SPO data"""
        try:
            # Clean response
            cleaned_response = self._clean_llm_response(response)
            
            # Parse JSON
            spo_data = json.loads(cleaned_response)
            
            # Validate required fields
            required_fields = ['attributes', 'triples', 'entity_types']
            for field in required_fields:
                if field not in spo_data:
                    logger.warning(f"âš ï¸ ç¼ºå°‘å­—æ®µ: {field}")
                    spo_data[field] = {} if field != 'triples' else []
            
            return spo_data
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
            logger.debug(f"åŸå§‹å“åº”: {response}")
            return {"attributes": {}, "triples": [], "entity_types": {}}
    
    def _clean_llm_response(self, response: str) -> str:
        """Clean LLM response to extract JSON"""
        # Remove markdown code blocks
        response = response.strip()
        if response.startswith('```json'):
            response = response[7:]
        elif response.startswith('```'):
            response = response[3:]
        if response.endswith('```'):
            response = response[:-3]
        
        # Find JSON content - look for the first complete JSON object
        start_idx = response.find('{')
        if start_idx == -1:
            return "{}"
        
        # Find the matching closing brace
        brace_count = 0
        end_idx = start_idx
        
        for i in range(start_idx, len(response)):
            if response[i] == '{':
                brace_count += 1
            elif response[i] == '}':
                brace_count -= 1
                if brace_count == 0:
                    end_idx = i
                    break
        
        if brace_count == 0:
            json_content = response[start_idx:end_idx+1]
        else:
            # Fallback to original method
            end_idx = response.rfind('}')
            if end_idx > start_idx:
                json_content = response[start_idx:end_idx+1]
            else:
                json_content = "{}"
        
        return json_content.strip()
    
    def _convert_spo_to_objects(self, spo_data: Dict[str, Any], source_text: str, **kwargs) -> Tuple[List[Entity], List[Relationship]]:
        """Convert SPO data to Entity and Relationship objects"""
        entities = []
        relationships = []
        entity_id_map = {}  # name -> id mapping
        
        # Create entities
        entity_types = spo_data.get('entity_types', {})
        attributes = spo_data.get('attributes', {})
        
        for entity_name, entity_type in entity_types.items():
            # Generate unique ID
            entity_id = f"entity_{len(entities) + 1}"
            entity_id_map[entity_name] = entity_id
            
            # Get entity attributes
            entity_attrs = attributes.get(entity_name, [])
            attr_dict = {}
            description_parts = []
            
            for attr in entity_attrs:
                if ':' in attr:
                    key, value = attr.split(':', 1)
                    attr_dict[key.strip()] = value.strip()
                    description_parts.append(attr)
                else:
                    description_parts.append(attr)
            
            # Create entity
            try:
                entity_type_enum = EntityType(entity_type.lower())
            except ValueError:
                entity_type_enum = EntityType.CONCEPT  # Default fallback
            
            entity = Entity(
                id=entity_id,
                name=entity_name,
                entity_type=entity_type_enum,
                description='; '.join(description_parts),
                confidence=0.8,  # Default confidence
                attributes=attr_dict,
                source_chunks={kwargs.get('chunk_id', 'unknown')}
            )
            
            entities.append(entity)
            logger.debug(f"ğŸ“ åˆ›å»ºå®ä½“: {entity_name} ({entity_type}) -> {entity_id}")
        
        # Create relationships
        triples = spo_data.get('triples', [])
        
        for triple in triples:
            if len(triple) != 3:
                logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆä¸‰å…ƒç»„: {triple}")
                continue
            
            source_name, relation, target_name = triple
            
            # Get entity IDs with fuzzy matching
            source_id = self._find_entity_id(source_name, entity_id_map)
            target_id = self._find_entity_id(target_name, entity_id_map)
            
            if not source_id:
                # åŠ¨æ€åˆ›å»ºç¼ºå¤±çš„æºå®ä½“
                source_id = self._create_missing_entity(source_name, entities, entity_id_map)
                if not source_id:
                    logger.warning(f"âš ï¸ æºå®ä½“æœªæ‰¾åˆ°ä¸”æ— æ³•åˆ›å»º: {source_name}")
                    continue
                else:
                    logger.info(f"ğŸ”§ åŠ¨æ€åˆ›å»ºæºå®ä½“: {source_name}")
                    
            if not target_id:
                # åŠ¨æ€åˆ›å»ºç¼ºå¤±çš„ç›®æ ‡å®ä½“
                target_id = self._create_missing_entity(target_name, entities, entity_id_map)
                if not target_id:
                    logger.warning(f"âš ï¸ ç›®æ ‡å®ä½“æœªæ‰¾åˆ°ä¸”æ— æ³•åˆ›å»º: {target_name}")
                    continue
                else:
                    logger.info(f"ğŸ”§ åŠ¨æ€åˆ›å»ºç›®æ ‡å®ä½“: {target_name}")
            
            # Create relationship
            try:
                relation_type_enum = RelationType(relation.lower().replace(' ', '_'))
            except ValueError:
                relation_type_enum = RelationType.RELATED_TO  # Default fallback
            
            relationship = Relationship(
                source_entity_id=source_id,
                target_entity_id=target_id,
                relation_type=relation_type_enum,
                description=f"{source_name} {relation} {target_name}",
                confidence=0.8,  # Default confidence
                source_chunks={kwargs.get('chunk_id', 'unknown')}
            )
            
            relationships.append(relationship)
            logger.debug(f"ğŸ”— åˆ›å»ºå…³ç³»: {source_name} --[{relation}]--> {target_name}")
        
        return entities, relationships