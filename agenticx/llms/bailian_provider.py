from typing import Any, Optional, Dict, List, AsyncGenerator, Generator, Union
import openai
import json
from pydantic import Field
from loguru import logger
from .base import BaseLLMProvider
from .response import LLMResponse, TokenUsage, LLMChoice

class BailianProvider(BaseLLMProvider):
    """
    Bailian (Dashscope) LLM provider that uses OpenAI-compatible API.
    Supports the latest Bailian models through Aliyun's API.
    """
    
    api_key: str = Field(description="Bailian API key")
    base_url: str = Field(default="https://dashscope.aliyuncs.com/compatible-mode/v1", description="Bailian API base URL")
    timeout: Optional[float] = Field(default=60.0, description="Request timeout in seconds")
    max_retries: Optional[int] = Field(default=3, description="Maximum number of retries")
    temperature: Optional[float] = Field(default=0.6, description="Sampling temperature")
    client: Optional[Any] = Field(default=None, exclude=True, description="OpenAI client instance")
    
    def __init__(self, **data):
        super().__init__(**data)
        self.client = openai.OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            timeout=self.timeout,
            max_retries=self.max_retries or 3
        )
    
    def invoke(
        self, prompt: Union[str, List[Dict]], tools: Optional[List[Dict]] = None, **kwargs
    ) -> LLMResponse:
        """Invoke the Bailian model synchronously."""
        try:
            # Convert prompt to messages format
            if isinstance(prompt, str):
                messages = [{"role": "user", "content": prompt}]
            elif isinstance(prompt, list):
                messages = prompt
            else:
                raise ValueError("Prompt must be either a string or a list of message dictionaries")
            
            request_params = {
                "model": self.model,
                "messages": messages,
                "temperature": kwargs.get("temperature", self.temperature),
                **kwargs
            }
            
            if tools:
                request_params["tools"] = tools
            
            # è®°å½•è¯·æ±‚è¯¦æƒ…
            logger.info(f"ğŸ¤– å‘é€è¯·æ±‚åˆ°ç™¾ç‚¼API")
            logger.debug(f"ğŸ“ æ¨¡å‹: {self.model}")
            logger.debug(f"ğŸŒ¡ï¸ æ¸©åº¦: {request_params.get('temperature', self.temperature)}")
            logger.debug(f"ğŸ’¬ æ¶ˆæ¯æ•°é‡: {len(messages)}")
            
            # è®°å½•æ¶ˆæ¯å†…å®¹ï¼ˆæˆªæ–­é•¿æ¶ˆæ¯ï¼‰
            for i, msg in enumerate(messages):
                content = msg.get('content', '')
                if isinstance(content, str):
                    content_preview = content[:200] + "..." if len(content) > 200 else content
                    logger.debug(f"ğŸ“¨ æ¶ˆæ¯[{i}] ({msg.get('role', 'unknown')}): {content_preview}")
                else:
                    logger.debug(f"ğŸ“¨ æ¶ˆæ¯[{i}] ({msg.get('role', 'unknown')}): [å¤æ‚å†…å®¹]")
            
            if tools:
                logger.debug(f"ğŸ”§ å·¥å…·æ•°é‡: {len(tools)}")
            
            # è®°å½•å®Œæ•´è¯·æ±‚å‚æ•°ï¼ˆè°ƒè¯•çº§åˆ«ï¼‰
            logger.trace(f"ğŸ” å®Œæ•´è¯·æ±‚å‚æ•°: {json.dumps(request_params, ensure_ascii=False, indent=2)}")
            
            if self.client is None:
                raise ValueError("Client not initialized")
            
            logger.debug("â³ æ­£åœ¨è°ƒç”¨ç™¾ç‚¼API...")
            logger.debug(f"ğŸ” æœ€ç»ˆè¯·æ±‚å‚æ•°: {list(request_params.keys())}")
            
            response = self.client.chat.completions.create(**request_params)
            
            # è®°å½•å“åº”è¯¦æƒ…
            logger.info("âœ… ç™¾ç‚¼APIå“åº”æˆåŠŸ")
            if hasattr(response, 'usage') and response.usage:
                logger.debug(f"ğŸ“Š Tokenä½¿ç”¨æƒ…å†µ:")
                logger.debug(f"  - è¾“å…¥Token: {response.usage.prompt_tokens}")
                logger.debug(f"  - è¾“å‡ºToken: {response.usage.completion_tokens}")
                logger.debug(f"  - æ€»Token: {response.usage.total_tokens}")
            
            if hasattr(response, 'choices') and response.choices:
                choice = response.choices[0]
                if hasattr(choice, 'message') and choice.message:
                    content = choice.message.content or ""
                    content_preview = content[:300] + "..." if len(content) > 300 else content
                    logger.debug(f"ğŸ’¬ å“åº”å†…å®¹é¢„è§ˆ: {content_preview}")
                    logger.debug(f"ğŸ“ å“åº”é•¿åº¦: {len(content)} å­—ç¬¦")
            
            # è®°å½•å®Œæ•´å“åº”ï¼ˆtraceçº§åˆ«ï¼‰
            logger.trace(f"ğŸ” å®Œæ•´APIå“åº”: {response}")
            
            parsed_response = self._parse_response(response)
            logger.debug(f"âœ¨ å“åº”è§£æå®Œæˆ")
            return parsed_response
        except Exception as e:
            logger.error(f"âŒ ç™¾ç‚¼APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise Exception(f"Bailian API call failed: {str(e)}")
    
    async def ainvoke(
        self, prompt: Union[str, List[Dict]], tools: Optional[List[Dict]] = None, **kwargs
    ) -> LLMResponse:
        """Invoke the Bailian model asynchronously."""
        try:
            # Convert prompt to messages format
            if isinstance(prompt, str):
                messages = [{"role": "user", "content": prompt}]
            elif isinstance(prompt, list):
                messages = prompt
            else:
                raise ValueError("Prompt must be either a string or a list of message dictionaries")
            
            async_client = openai.AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.base_url,
                timeout=self.timeout,
                max_retries=self.max_retries or 3
            )
            
            request_params = {
                "model": self.model,
                "messages": messages,
                "temperature": kwargs.get("temperature", self.temperature),
                **kwargs
            }
            
            if tools:
                request_params["tools"] = tools
            
            response = await async_client.chat.completions.create(**request_params)
            return self._parse_response(response)
        except Exception as e:
            raise Exception(f"Bailian API async call failed: {str(e)}")
    
    def stream(self, prompt: Union[str, List[Dict]], **kwargs) -> Generator[str, None, None]:
        """Stream the Bailian model's response synchronously."""
        try:
            # Convert prompt to messages format
            if isinstance(prompt, str):
                messages = [{"role": "user", "content": prompt}]
            elif isinstance(prompt, list):
                messages = prompt
            else:
                raise ValueError("Prompt must be either a string or a list of message dictionaries")
            
            request_params = {
                "model": self.model,
                "messages": messages,
                "temperature": kwargs.get("temperature", self.temperature),
                "stream": True,
                **kwargs
            }
            
            if self.client is None:
                raise ValueError("Client not initialized")
                
            response_stream = self.client.chat.completions.create(**request_params)
            
            for chunk in response_stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            raise Exception(f"Bailian API stream call failed: {str(e)}")
    
    async def astream(self, prompt: Union[str, List[Dict]], **kwargs):  # type: ignore
        """Stream the Bailian model's response asynchronously."""
        try:
            # Convert prompt to messages format
            if isinstance(prompt, str):
                messages = [{"role": "user", "content": prompt}]
            elif isinstance(prompt, list):
                messages = prompt
            else:
                raise ValueError("Prompt must be either a string or a list of message dictionaries")
            
            async_client = openai.AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.base_url,
                timeout=self.timeout,
                max_retries=self.max_retries or 3
            )
            
            request_params = {
                "model": self.model,
                "messages": messages,
                "temperature": kwargs.get("temperature", self.temperature),
                "stream": True,
                **kwargs
            }
            
            response_stream = await async_client.chat.completions.create(**request_params)
            
            async for chunk in response_stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            raise Exception(f"Bailian API async stream call failed: {str(e)}")
    
    def _parse_response(self, response) -> LLMResponse:
        """Parse OpenAI response into AgenticX LLMResponse format."""
        usage = response.usage
        token_usage = TokenUsage(
            prompt_tokens=usage.prompt_tokens if usage else 0,
            completion_tokens=usage.completion_tokens if usage else 0,
            total_tokens=usage.total_tokens if usage else 0
        )
        
        choices = [
            LLMChoice(
                index=choice.index,
                content=choice.message.content or "",
                finish_reason=choice.finish_reason
            ) for choice in response.choices
        ]
        
        main_content = choices[0].content if choices else ""
        
        return LLMResponse(
            id=response.id,
            model_name=response.model,
            created=response.created,
            content=main_content,
            choices=choices,
            token_usage=token_usage,
            cost=None,
            metadata={
                "provider": "bailian",
                "api_version": "v1"
            }
        )
    
    def generate(self, prompt: str, **kwargs) -> str:
        """Generate text response from a simple prompt string.
        
        Args:
            prompt: The input prompt string
            **kwargs: Additional generation parameters
            
        Returns:
            Generated text content as string
        """
        response = self.invoke(prompt, **kwargs)
        return response.content

    def call(self, prompt: Union[str, List[Dict]], **kwargs) -> str:
        """Call method for compatibility with extractors.
        
        Args:
            prompt: The input prompt
            **kwargs: Additional parameters
            
        Returns:
            Generated text content as string
        """
        logger.debug("ğŸ”„ è°ƒç”¨callæ–¹æ³•ï¼ˆå…¼å®¹æ€§æ¥å£ï¼‰")
        response = self.invoke(prompt, **kwargs)
        logger.debug(f"ğŸ“¤ è¿”å›æ–‡æœ¬å†…å®¹ï¼Œé•¿åº¦: {len(response.content)} å­—ç¬¦")
        return response.content

    @classmethod
    def from_config(cls, config: Dict[str, Any]) -> "BailianProvider":
        """Create BailianProvider from configuration dictionary."""
        return cls(
            model=config.get("model", "qwen-plus"),
            api_key=config.get("api_key"),
            base_url=config.get("base_url", "https://dashscope.aliyuncs.com/compatible-mode/v1"),
            timeout=config.get("timeout", 60.0),
            max_retries=config.get("max_retries", 3),
            temperature=config.get("temperature", 0.6)
        )

    def create_multimodal_message(self, text: str, image_url: Optional[str] = None, 
                                image_base64: Optional[str] = None) -> Dict:
        """åˆ›å»ºå¤šæ¨¡æ€æ¶ˆæ¯æ ¼å¼
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            image_url: å›¾ç‰‡URLï¼ˆå¯é€‰ï¼‰
            image_base64: Base64ç¼–ç çš„å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ ¼å¼åŒ–çš„å¤šæ¨¡æ€æ¶ˆæ¯
        """
        content: List[Dict[str, Any]] = [{"type": "text", "text": text}]
        
        if image_url:
            content.append({
                "type": "image_url",
                "image_url": {"url": image_url}
            })
        elif image_base64:
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
            })
            
        return {"role": "user", "content": content}