"""
AgenticX M8 & M9 ç®€åŒ–æ¼”ç¤ºï¼šä»»åŠ¡éªŒè¯ + å·¥ä½œæµç¼–æ’

è¿™ä¸ªæ¼”ç¤ºå±•ç¤ºäº† M8 å’Œ M9 çš„æ ¸å¿ƒåŠŸèƒ½ï¼š
1. M8 ä»»åŠ¡è¾“å‡ºè§£æå’ŒéªŒè¯
2. M9 å·¥ä½œæµç¼–æ’å’Œæ¡ä»¶è·¯ç”±
3. äº‹ä»¶é©±åŠ¨çš„è§¦å‘å™¨

ä½¿ç”¨ç®€å•çš„è®¡ç®—ä»»åŠ¡æ¥å±•ç¤ºå·¥ä½œæµçš„æ‰§è¡Œè¿‡ç¨‹ã€‚
"""

import sys
import os
import json
import asyncio
from typing import Dict, Any
from pydantic import BaseModel, Field
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from agenticx.core import (
    tool, WorkflowEngine, WorkflowGraph,
    TaskOutputParser, TaskResultValidator, OutputRepairLoop,
    RepairStrategy, WorkflowStatus
)


# å®šä¹‰æ•°æ®æ¨¡å‹
class CalculationResult(BaseModel):
    """è®¡ç®—ç»“æœæ¨¡å‹"""
    operation: str = Field(..., description="æ‰§è¡Œçš„æ“ä½œ")
    input_values: list = Field(..., description="è¾“å…¥å€¼")
    result: float = Field(..., description="è®¡ç®—ç»“æœ")
    timestamp: str = Field(..., description="è®¡ç®—æ—¶é—´")


class ProcessingSummary(BaseModel):
    """å¤„ç†æ‘˜è¦æ¨¡å‹"""
    total_operations: int = Field(..., description="æ€»æ“ä½œæ•°")
    results: list = Field(..., description="æ‰€æœ‰ç»“æœ")
    final_result: float = Field(..., description="æœ€ç»ˆç»“æœ")
    status: str = Field(..., description="å¤„ç†çŠ¶æ€")


# å®šä¹‰å·¥ä½œæµå·¥å…·
@tool()
def add_numbers(a: float = 10.0, b: float = 5.0) -> str:
    """åŠ æ³•è¿ç®—"""
    result = {
        "operation": "addition",
        "input_values": [a, b],
        "result": a + b,
        "timestamp": datetime.now().isoformat()
    }
    return json.dumps(result, ensure_ascii=False)


@tool()
def multiply_numbers(a: float = 2.0, b: float = 3.0) -> str:
    """ä¹˜æ³•è¿ç®—"""
    result = {
        "operation": "multiplication", 
        "input_values": [a, b],
        "result": a * b,
        "timestamp": datetime.now().isoformat()
    }
    return json.dumps(result, ensure_ascii=False)


@tool()
def process_calculation_result(calc_result: str) -> str:
    """å¤„ç†è®¡ç®—ç»“æœ"""
    try:
        data = json.loads(calc_result)
        processed = {
            "operation": data["operation"],
            "result": data["result"],
            "processed_at": datetime.now().isoformat(),
            "is_positive": data["result"] > 0,
            "magnitude": "large" if abs(data["result"]) > 10 else "small"
        }
        return json.dumps(processed, ensure_ascii=False)
    except Exception as e:
        return json.dumps({"error": f"å¤„ç†å¤±è´¥: {str(e)}"}, ensure_ascii=False)


@tool()
def combine_results(result1: str, result2: str) -> str:
    """åˆå¹¶å¤šä¸ªç»“æœ"""
    try:
        data1 = json.loads(result1)
        data2 = json.loads(result2)
        
        combined = {
            "total_operations": 2,
            "results": [data1, data2],
            "final_result": data1.get("result", 0) + data2.get("result", 0),
            "status": "completed"
        }
        return json.dumps(combined, ensure_ascii=False)
    except Exception as e:
        return json.dumps({"error": f"åˆå¹¶å¤±è´¥: {str(e)}"}, ensure_ascii=False)


@tool()
def generate_summary(combined_result: str) -> str:
    """ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š"""
    try:
        data = json.loads(combined_result)
        
        summary = {
            "workflow_summary": "æ•°å­¦è®¡ç®—å·¥ä½œæµæ‰§è¡Œå®Œæˆ",
            "total_operations": data.get("total_operations", 0),
            "final_result": data.get("final_result", 0),
            "execution_time": datetime.now().isoformat(),
            "status": "success"
        }
        
        # ä¿å­˜æ‘˜è¦åˆ°æ–‡ä»¶
        filename = f"calculation_summary_{int(datetime.now().timestamp())}.json"
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        summary["report_file"] = filename
        return json.dumps(summary, ensure_ascii=False)
    except Exception as e:
        return json.dumps({"error": f"ç”Ÿæˆæ‘˜è¦å¤±è´¥: {str(e)}"}, ensure_ascii=False)


class SimpleWorkflowDemo:
    """ç®€å•å·¥ä½œæµæ¼”ç¤º"""
    
    def __init__(self):
        self.engine = WorkflowEngine(max_concurrent_nodes=3)
        self.parser = TaskOutputParser(enable_fuzzy_parsing=True)
        self.validator = TaskResultValidator()
        self.repair_loop = OutputRepairLoop(
            max_repair_attempts=2,
            repair_strategy=RepairStrategy.SIMPLE
        )
    
    def create_sequential_workflow(self) -> WorkflowGraph:
        """åˆ›å»ºé¡ºåºæ‰§è¡Œå·¥ä½œæµ"""
        graph = WorkflowGraph()
        
        # æ·»åŠ èŠ‚ç‚¹
        graph.add_node("add_step", add_numbers, "tool", {
            "description": "æ‰§è¡ŒåŠ æ³•è¿ç®—",
            "args": {"a": 15.0, "b": 25.0}
        })
        
        graph.add_node("multiply_step", multiply_numbers, "tool", {
            "description": "æ‰§è¡Œä¹˜æ³•è¿ç®—", 
            "args": {"a": 4.0, "b": 7.0}
        })
        
        graph.add_node("process_add", process_calculation_result, "tool", {
            "description": "å¤„ç†åŠ æ³•ç»“æœ",
            "args": {"calc_result": "${add_step}"}
        })
        
        graph.add_node("process_multiply", process_calculation_result, "tool", {
            "description": "å¤„ç†ä¹˜æ³•ç»“æœ",
            "args": {"calc_result": "${multiply_step}"}
        })
        
        graph.add_node("combine_step", combine_results, "tool", {
            "description": "åˆå¹¶è®¡ç®—ç»“æœ",
            "args": {
                "result1": "${process_add}",
                "result2": "${process_multiply}"
            }
        })
        
        graph.add_node("summary_step", generate_summary, "tool", {
            "description": "ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š",
            "args": {"combined_result": "${combine_step}"}
        })
        
        # æ·»åŠ è¾¹ï¼ˆå®šä¹‰æ‰§è¡Œé¡ºåºï¼‰
        graph.add_edge("add_step", "process_add")
        graph.add_edge("multiply_step", "process_multiply")
        graph.add_edge("process_add", "combine_step")
        graph.add_edge("process_multiply", "combine_step")
        graph.add_edge("combine_step", "summary_step")
        
        return graph
    
    def create_conditional_workflow(self) -> WorkflowGraph:
        """åˆ›å»ºæ¡ä»¶è·¯ç”±å·¥ä½œæµ"""
        graph = WorkflowGraph()
        
        # æ·»åŠ èŠ‚ç‚¹
        graph.add_node("initial_calc", add_numbers, "tool", {
            "description": "åˆå§‹è®¡ç®—",
            "args": {"a": 8.0, "b": 12.0}
        })
        
        # æ¡ä»¶åˆ†æ”¯ï¼šæ ¹æ®ç»“æœé€‰æ‹©ä¸åŒçš„å¤„ç†è·¯å¾„
        graph.add_node("small_number_processing", multiply_numbers, "tool", {
            "description": "å°æ•°å¤„ç†",
            "args": {"a": 2.0, "b": 3.0}
        })
        
        graph.add_node("large_number_processing", multiply_numbers, "tool", {
            "description": "å¤§æ•°å¤„ç†", 
            "args": {"a": 10.0, "b": 20.0}
        })
        
        graph.add_node("final_processing", process_calculation_result, "tool", {
            "description": "æœ€ç»ˆå¤„ç†",
            "args": {"calc_result": "${small_number_processing}${large_number_processing}"}
        })
        
        # æ·»åŠ æ¡ä»¶è¾¹
        graph.add_edge("initial_calc", "small_number_processing", 
                      lambda result: self._get_result_value(result) < 15)
        
        graph.add_edge("initial_calc", "large_number_processing",
                      lambda result: self._get_result_value(result) >= 15)
        
        graph.add_edge("small_number_processing", "final_processing")
        graph.add_edge("large_number_processing", "final_processing")
        
        return graph
    
    def _get_result_value(self, result: str) -> float:
        """ä»ç»“æœä¸­æå–æ•°å€¼"""
        try:
            data = json.loads(result)
            return data.get("result", 0)
        except:
            return 0
    
    def demonstrate_task_validation(self):
        """æ¼”ç¤ºä»»åŠ¡éªŒè¯åŠŸèƒ½"""
        print("ğŸ” M8 ä»»åŠ¡éªŒè¯æ¼”ç¤º")
        print("=" * 50)
        
        # æµ‹è¯•æ­£ç¡®çš„è¾“å‡º
        print("\nâœ… æµ‹è¯•æ­£ç¡®çš„JSONè¾“å‡º:")
        correct_output = '{"operation": "test", "result": 42.0, "timestamp": "2024-01-01T00:00:00"}'
        parse_result = self.parser.parse(correct_output, CalculationResult)
        print(f"  è§£æç»“æœ: {'æˆåŠŸ' if parse_result.success else 'å¤±è´¥'}")
        
        if parse_result.success:
            validation_result = self.validator.validate(parse_result.data)
            print(f"  éªŒè¯ç»“æœ: {'é€šè¿‡' if validation_result.valid else 'å¤±è´¥'}")
        
        # æµ‹è¯•éœ€è¦ä¿®å¤çš„è¾“å‡º
        print("\nğŸ”§ æµ‹è¯•éœ€è¦ä¿®å¤çš„è¾“å‡º:")
        malformed_output = "{'operation': 'test', 'result': 42.0, 'timestamp': '2024-01-01T00:00:00'}"
        parse_result = self.parser.parse(malformed_output, CalculationResult)
        print(f"  åˆå§‹è§£æ: {'æˆåŠŸ' if parse_result.success else 'å¤±è´¥'}")
        
        if not parse_result.success:
            repaired_result = self.repair_loop.repair(
                malformed_output, parse_result, None, CalculationResult
            )
            print(f"  ä¿®å¤ç»“æœ: {'æˆåŠŸ' if repaired_result.success else 'å¤±è´¥'}")
        
        print()
    
    async def run_sequential_workflow(self):
        """è¿è¡Œé¡ºåºå·¥ä½œæµ"""
        print("ğŸš€ M9 é¡ºåºå·¥ä½œæµæ¼”ç¤º")
        print("=" * 50)
        
        graph = self.create_sequential_workflow()
        
        start_time = datetime.now()
        context = await self.engine.run(
            graph,
            {"workflow_id": f"sequential_{int(start_time.timestamp())}"}
        )
        end_time = datetime.now()
        
        print(f"\nğŸ“Š æ‰§è¡Œç»“æœ:")
        print(f"  çŠ¶æ€: {'âœ… æˆåŠŸ' if context.status == WorkflowStatus.COMPLETED else 'âŒ å¤±è´¥'}")
        print(f"  æ‰§è¡Œæ—¶é—´: {(end_time - start_time).total_seconds():.3f} ç§’")
        print(f"  æ‰§è¡ŒèŠ‚ç‚¹æ•°: {len(context.node_results)}")
        
        # æ˜¾ç¤ºå…³é”®ç»“æœ
        if "add_step" in context.node_results:
            add_result = json.loads(context.node_results["add_step"])
            print(f"  åŠ æ³•ç»“æœ: {add_result['result']}")
        
        if "multiply_step" in context.node_results:
            multiply_result = json.loads(context.node_results["multiply_step"])
            print(f"  ä¹˜æ³•ç»“æœ: {multiply_result['result']}")
        
        return context
    
    async def run_conditional_workflow(self):
        """è¿è¡Œæ¡ä»¶å·¥ä½œæµ"""
        print("ğŸ”€ M9 æ¡ä»¶è·¯ç”±å·¥ä½œæµæ¼”ç¤º")
        print("=" * 50)
        
        graph = self.create_conditional_workflow()
        
        start_time = datetime.now()
        context = await self.engine.run(
            graph,
            {"workflow_id": f"conditional_{int(start_time.timestamp())}"}
        )
        end_time = datetime.now()
        
        print(f"\nğŸ“Š æ‰§è¡Œç»“æœ:")
        print(f"  çŠ¶æ€: {'âœ… æˆåŠŸ' if context.status == WorkflowStatus.COMPLETED else 'âŒ å¤±è´¥'}")
        print(f"  æ‰§è¡Œæ—¶é—´: {(end_time - start_time).total_seconds():.3f} ç§’")
        print(f"  æ‰§è¡ŒèŠ‚ç‚¹æ•°: {len(context.node_results)}")
        
        # åˆ†ææ‰§è¡Œè·¯å¾„
        if "small_number_processing" in context.node_results:
            print("  ğŸ›¤ï¸  æ‰§è¡Œè·¯å¾„: å°æ•°å¤„ç†åˆ†æ”¯")
        elif "large_number_processing" in context.node_results:
            print("  ğŸ›¤ï¸  æ‰§è¡Œè·¯å¾„: å¤§æ•°å¤„ç†åˆ†æ”¯")
        
        return context


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ AgenticX M8 & M9 ç®€åŒ–æ¼”ç¤º")
    print("ä»»åŠ¡éªŒè¯ + å·¥ä½œæµç¼–æ’æ ¸å¿ƒåŠŸèƒ½å±•ç¤º")
    print("=" * 60)
    
    demo = SimpleWorkflowDemo()
    
    try:
        # 1. æ¼”ç¤ºä»»åŠ¡éªŒè¯
        demo.demonstrate_task_validation()
        
        # 2. æ¼”ç¤ºé¡ºåºå·¥ä½œæµ
        await demo.run_sequential_workflow()
        
        print("\n" + "=" * 60)
        
        # 3. æ¼”ç¤ºæ¡ä»¶å·¥ä½œæµ
        await demo.run_conditional_workflow()
        
        # 4. æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶
        print("\nğŸ“„ ç”Ÿæˆçš„æ–‡ä»¶:")
        import glob
        summary_files = glob.glob("calculation_summary_*.json")
        for file in summary_files:
            print(f"  âœ… {file}")
        
        print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ“‹ åŠŸèƒ½æ€»ç»“:")
        print("  âœ… M8 ä»»åŠ¡éªŒè¯ï¼šJSONè§£æã€æ ¼å¼ä¿®å¤ã€SchemaéªŒè¯")
        print("  âœ… M9 å·¥ä½œæµç¼–æ’ï¼šé¡ºåºæ‰§è¡Œã€å¹¶è¡Œå¤„ç†ã€æ¡ä»¶è·¯ç”±")
        print("  âœ… å˜é‡è§£æï¼šèŠ‚ç‚¹é—´æ•°æ®ä¼ é€’")
        print("  âœ… é”™è¯¯å¤„ç†ï¼šä¼˜é›…é™çº§å’Œæ¢å¤")
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main()) 