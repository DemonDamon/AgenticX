"""
AI åŒ»ç–—åŠ©æ‰‹æ¼”ç¤ºè„šæœ¬

å±•ç¤ºå¦‚ä½•ä½¿ç”¨ AgenticX å’Œæ·±åº¦é›†æˆçš„ Mem0 è®°å¿†ç»„ä»¶
æ„å»ºä¸€ä¸ªèƒ½å¤Ÿè®°å¿†æ‚£è€…ä¿¡æ¯å¹¶æä¾›ä¸ªæ€§åŒ–å»ºè®®çš„ AI åŒ»ç–—åŠ©æ‰‹ã€‚
"""

import asyncio
import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from agenticx.llms import LiteLLMProvider
from agenticx.memory.mem0_wrapper import Mem0


class HealthcareAgent:
    """
    ä¸€ä¸ªèƒ½å¤Ÿè®°å¿†æ‚£è€…ä¸Šä¸‹æ–‡çš„ AI åŒ»ç–—åŠ©æ‰‹ã€‚
    """
    
    def __init__(self, llm_provider):
        """
        åˆå§‹åŒ–åŒ»ç–—åŠ©æ‰‹ã€‚

        :param llm_provider: ä¸€ä¸ªå®ç°äº† agenticx.llms.base.BaseLLM çš„ LLM æä¾›è€…å®ä¾‹ã€‚
        """
        self.llm = llm_provider
        
        # ä½¿ç”¨ AgenticX çš„ LLM å®ä¾‹æ¥åˆå§‹åŒ–æˆ‘ä»¬çš„æ–° Mem0 ç»„ä»¶
        self.memory = Mem0(llm=self.llm)
        print("âœ… AI åŒ»ç–—åŠ©æ‰‹å·²åˆå§‹åŒ–ï¼Œå¹¶é…å¤‡äº†ç”± AgenticX LLM é©±åŠ¨çš„é•¿æœŸè®°å¿†ã€‚")
    
    async def initial_consultation(self, patient_info: str, patient_id: str):
        """
        è¿›è¡Œåˆæ¬¡é—®è¯Šï¼Œè®°å½•æ‚£è€…çš„åŸºæœ¬ä¿¡æ¯å’Œä¸»è¯‰ã€‚

        :param patient_info: æ‚£è€…çš„å£è¿°ä¿¡æ¯ã€‚
        :param patient_id: æ‚£è€…çš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚
        """
        print(f"\nğŸ©º æ­£åœ¨ä¸ºæ‚£è€… {patient_id} è¿›è¡Œåˆæ¬¡é—®è¯Š...")
        print(f"   æ‚£è€…ä¸»è¯‰: '{patient_info}'")
        
        # å°†ä¿¡æ¯å­˜å…¥é•¿æœŸè®°å¿†ï¼Œè¿™é‡Œçš„å…ƒæ•°æ®å¯¹äºåŒºåˆ†ä¸åŒæ‚£è€…è‡³å…³é‡è¦
        await asyncio.to_thread(
            self.memory.add,
            content=patient_info,
            metadata={"user_id": patient_id, "session_type": "initial_consultation"}
        )
        print(f"   [è®°å¿†æ“ä½œ] å·²å°†æ‚£è€… {patient_id} çš„ä¿¡æ¯å­˜å…¥é•¿æœŸè®°å¿†ã€‚")
        
        # æ¨¡æ‹Ÿ LLM ç”Ÿæˆå›åº”
        response = await self.llm.ainvoke([
            {"role": "system", "content": "You are a helpful healthcare assistant. Acknowledge the patient's statement and confirm you've noted it."},
            {"role": "user", "content": patient_info}
        ])
        
        print(f"âœ… åˆè¯Šå®Œæˆã€‚")
        return response.content

    async def follow_up_question(self, question: str, patient_id: str) -> str:
        """
        å›ç­”æ‚£è€…çš„åç»­é—®é¢˜ï¼Œä¼šåˆ©ç”¨ä¹‹å‰å­˜å‚¨çš„è®°å¿†ã€‚

        :param question: æ‚£è€…çš„åç»­é—®é¢˜ã€‚
        :param patient_id: æ‚£è€…çš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚
        """
        print(f"\nâ“ æ‚£è€… {patient_id} æé—®: '{question}'")
        
        print(f"   [è®°å¿†æ“ä½œ] æ­£åœ¨æœç´¢æ‚£è€… {patient_id} çš„ç›¸å…³ç—…å²...")
        # æœç´¢ä¸è¯¥æ‚£è€…ç›¸å…³çš„è®°å¿†
        search_results = await asyncio.to_thread(
            self.memory.get,
            query=question,
            metadata={"user_id": patient_id}
        )
        
        context = "No relevant past information found."
        if search_results and search_results.get("results"):
            context = "\n".join([result["memory"] for result in search_results["results"]])
            print(f"   [è®°å¿†æ“ä½œ] æ‰¾åˆ°ç›¸å…³è®°å¿†: {context}")
        else:
            print("   [è®°å¿†æ“ä½œ] æœªæ‰¾åˆ°ç›¸å…³è®°å¿†ã€‚")

        # å°†æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å’Œæ–°é—®é¢˜ä¸€èµ·å‘é€ç»™ LLM
        prompt = [
            {"role": "system", "content": f"You are a helpful healthcare assistant. Here is the patient's history you remember:\n---\n{context}\n---\nNow, answer the patient's new question based on their history."},
            {"role": "user", "content": question}
        ]
        
        response = await self.llm.ainvoke(prompt)
        print("âœ… å·²ç”Ÿæˆå›ç­”ã€‚")
        return response.content


async def main():
    """ä¸»å‡½æ•°ï¼Œè¿è¡Œæ¼”ç¤º"""
    print("ğŸš€ AI åŒ»ç–—åŠ©æ‰‹æ¼”ç¤ºï¼ˆä½¿ç”¨ Mem0 é›†æˆï¼‰")
    print("=" * 60)
    
    # åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # ç¡®ä¿è®¾ç½®äº† OPENAI_API_KEY ç¯å¢ƒå˜é‡
    api_key = os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("OPENAI_API_BASE") # è·å– base_url

    if not api_key:
        print("\nâŒ é”™è¯¯: è¯·åœ¨ .env æ–‡ä»¶ä¸­æˆ–ç¯å¢ƒä¸­è®¾ç½® OPENAI_API_KEYã€‚")
        return

    # 1. åˆå§‹åŒ– LLM Provider
    # è¿™é‡Œä½¿ç”¨ OpenAIï¼Œä½†å¯ä»¥æ˜¯ä»»ä½• AgenticX æ”¯æŒçš„ LLM
    print(f"ğŸŒ ä½¿ç”¨ API Base URL: {base_url or 'é»˜è®¤'}")
    llm = LiteLLMProvider(
        model="gpt-4o",
        api_key=api_key,
        base_url=base_url # ä¼ é€’ base_url
    )
    
    # 2. åˆ›å»ºåŒ»ç–—åŠ©æ‰‹å®ä¾‹
    assistant = HealthcareAgent(llm_provider=llm)
    
    patient_id = "patient_alex_456"
    
    # 3. æ¸…ç©ºè¯¥æ‚£è€…ä¹‹å‰çš„è®°å¿†ï¼Œç¡®ä¿æ¼”ç¤ºç¯å¢ƒå¹²å‡€
    print(f"\nğŸ§¹ å‡†å¤‡æ–°ä¼šè¯ï¼Œæ¸…ç©ºæ‚£è€… {patient_id} çš„è¿‡å¾€è®°å¿†...")
    # æ³¨æ„ï¼šåœ¨çœŸå®åº”ç”¨ä¸­ï¼Œä½ å¯èƒ½ä¸ä¼šéšæ„æ¸…ç©ºè®°å¿†
    await asyncio.to_thread(assistant.memory.clear)
    
    # 4. æ¨¡æ‹Ÿåˆæ¬¡é—®è¯Š
    initial_info = "ä½ å¥½ï¼Œæˆ‘å« Alexã€‚æˆ‘å¯¹é’éœ‰ç´ è¿‡æ•ï¼Œè€Œä¸”å¤´ç—›å·²ç»æŒç»­ä¸‰å¤©äº†ã€‚"
    response1 = await assistant.initial_consultation(initial_info, patient_id)
    print(f"\nğŸ¤– åŠ©æ‰‹å›åº”:\n{response1}")

    # 5. æ¨¡æ‹Ÿåç»­æé—®
    # è¿™ä¸ªé—®é¢˜ä¾èµ–äºåŠ©æ‰‹è®°å¾—æ‚£è€…å¯¹é’éœ‰ç´ è¿‡æ•
    follow_up_q = "æˆ‘å¤´ç—›å¾—å‰å®³ï¼Œå¯ä»¥åƒç‚¹é˜¿è«è¥¿æ—å—ï¼Ÿ"
    response2 = await assistant.follow_up_question(follow_up_q, patient_id)
    print(f"\nğŸ¤– åŠ©æ‰‹å›åº”:\n{response2}")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc() 