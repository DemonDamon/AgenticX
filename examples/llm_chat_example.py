#!/usr/bin/env python3
"""
AgenticX LLM Chat Example

ä¸€ä¸ªä½¿ç”¨ AgenticX LLM æ¨¡å—çš„èŠå¤©ç¤ºä¾‹è„šæœ¬ï¼Œæ”¯æŒï¼š
- å•æ¬¡è¡¥å…¨æ¨¡å¼
- äº¤äº’å¼å¯¹è¯æ¨¡å¼
- å¯é…ç½®çš„æ¨¡å‹é€‰æ‹©
- ç¯å¢ƒå˜é‡é…ç½®æ”¯æŒ
"""

import os
import sys
import argparse
from pathlib import Path
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from agenticx.llms import LiteLLMProvider


class AgenticXChatClient:
    """AgenticX èŠå¤©å®¢æˆ·ç«¯"""
    
    def __init__(self, model: str = "gpt-4o-mini"):
        """
        åˆå§‹åŒ–èŠå¤©å®¢æˆ·ç«¯
        
        Args:
            model: è¦ä½¿ç”¨çš„æ¨¡å‹åç§°
        """
        self.model = model
        self.provider = None
        self._setup_provider()
    
    def _setup_provider(self):
        """è®¾ç½® LLM æä¾›å•†"""
        # åŠ è½½ç¯å¢ƒå˜é‡ - é¦–å…ˆå°è¯•è„šæœ¬åŒçº§ç›®å½•çš„ .env æ–‡ä»¶
        script_dir = Path(__file__).parent
        env_file = script_dir / ".env"
        
        if env_file.exists():
            load_dotenv(env_file, override=True)
            print(f"âœ… ä» {env_file} åŠ è½½ç¯å¢ƒå˜é‡")
        else:
            # å¦‚æœè„šæœ¬åŒçº§ç›®å½•æ²¡æœ‰ .env æ–‡ä»¶ï¼Œå°è¯•å½“å‰å·¥ä½œç›®å½•
            load_dotenv(override=True)
            print("âœ… ä»å½“å‰å·¥ä½œç›®å½•åŠ è½½ç¯å¢ƒå˜é‡")
        
        # æ ¹æ®æ¨¡å‹åç§°è·å–å¯¹åº”çš„ç¯å¢ƒå˜é‡
        api_key = None
        base_url = None
        
        if self.model.startswith("deepseek/"):
            api_key = os.getenv("DEEPSEEK_API_KEY")
            base_url = os.getenv("DEEPSEEK_API_BASE")
            key_name = "DEEPSEEK_API_KEY"
        elif self.model.startswith("claude-") or self.model.startswith("anthropic/"):
            api_key = os.getenv("ANTHROPIC_API_KEY")
            base_url = os.getenv("ANTHROPIC_API_BASE")
            key_name = "ANTHROPIC_API_KEY"
        elif self.model.startswith("gemini/"):
            api_key = os.getenv("GOOGLE_API_KEY")
            base_url = os.getenv("GOOGLE_API_BASE")
            key_name = "GOOGLE_API_KEY"
        else:
            # é»˜è®¤ä½¿ç”¨ OpenAI é…ç½®
            api_key = os.getenv("OPENAI_API_KEY")
            base_url = os.getenv("OPENAI_API_BASE")
            key_name = "OPENAI_API_KEY"
        
        if not api_key:
            print(f"âŒ é”™è¯¯: è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® {key_name}")
            print(f"ğŸ’¡ è¯·ç¡®ä¿åœ¨ä»¥ä¸‹ä½ç½®ä¹‹ä¸€åˆ›å»º .env æ–‡ä»¶:")
            print(f"   1. è„šæœ¬åŒçº§ç›®å½•: {script_dir / '.env'}")
            print(f"   2. å½“å‰å·¥ä½œç›®å½•: {Path.cwd() / '.env'}")
            print("   å¹¶è®¾ç½®ä»¥ä¸‹å˜é‡:")
            print(f"   {key_name}=your_api_key_here")
            if key_name == "DEEPSEEK_API_KEY":
                print("   DEEPSEEK_API_BASE=your_base_url_here  # å¯é€‰ï¼Œç”¨äºä»£ç†")
            elif key_name == "ANTHROPIC_API_KEY":
                print("   ANTHROPIC_API_BASE=your_base_url_here  # å¯é€‰ï¼Œç”¨äºä»£ç†")
            elif key_name == "GOOGLE_API_KEY":
                print("   GOOGLE_API_BASE=your_base_url_here  # å¯é€‰ï¼Œç”¨äºä»£ç†")
            else:
                print("   OPENAI_API_BASE=your_base_url_here  # å¯é€‰ï¼Œç”¨äºä»£ç†")
            sys.exit(1)
        
        print(f"ğŸŒ ä½¿ç”¨ API Base URL: {base_url or 'é»˜è®¤ OpenAI URL'}")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {self.model}")
        
        # åˆ›å»º LLM æä¾›å•†
        self.provider = LiteLLMProvider(
            model=self.model,
            api_key=api_key,
            base_url=base_url
        )
    
    def completion(self, prompt: str) -> str:
        """
        å•æ¬¡è¡¥å…¨
        
        Args:
            prompt: è¾“å…¥æç¤º
            
        Returns:
            æ¨¡å‹çš„å›å¤
        """
        try:
            messages = [{"role": "user", "content": prompt}]
            response = self.provider.invoke(messages)
            return response.content
        except Exception as e:
            print(f"âŒ è¡¥å…¨å¤±è´¥: {e}")
            return ""
    
    def stream_completion(self, messages: list):
        """
        æµå¼è¡¥å…¨ç”Ÿæˆå™¨
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            
        Yields:
            å“åº”çš„æ–‡æœ¬å—
        """
        try:
            for chunk in self.provider.stream(messages):
                if chunk:
                    yield chunk
        except Exception as e:
            print(f"âŒ æµå¼è¡¥å…¨å¤±è´¥: {e}")
            return
    
    def interactive_chat(self):
        """äº¤äº’å¼å¯¹è¯æ¨¡å¼"""
        messages = []
        print(f"\nğŸš€ æ¬¢è¿ä½¿ç”¨ AgenticX {self.model} èŠå¤©ï¼")
        print("ğŸ’¡ è¾“å…¥ 'quit' é€€å‡º, è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å², è¾“å…¥ 'info' æŸ¥çœ‹å½“å‰é…ç½®")
        print("=" * 60)
        
        while True:
            try:
                user_input = input("\nğŸ‘¤ ä½ : ").strip()
                
                if user_input.lower() == 'quit':
                    print("\nğŸ‘‹ å†è§ï¼")
                    break
                elif user_input.lower() == 'clear':
                    messages = []
                    print("ğŸ§¹ å¯¹è¯å†å²å·²æ¸…ç©º")
                    continue
                elif user_input.lower() == 'info':
                    self._show_info(messages)
                    continue
                elif not user_input:
                    print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆå†…å®¹")
                    continue
                
                # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
                messages.append({"role": "user", "content": user_input})
                
                # è·å–å¹¶æ˜¾ç¤ºåŠ©æ‰‹å›å¤
                print("\nğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
                assistant_message = ""
                
                for chunk in self.stream_completion(messages):
                    if chunk:
                        print(chunk, end="", flush=True)
                        assistant_message += chunk
                
                print()  # æ¢è¡Œ
                
                # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
                if assistant_message:
                    messages.append({"role": "assistant", "content": assistant_message})
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
                break
            except Exception as e:
                print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
    
    def _show_info(self, messages: list):
        """æ˜¾ç¤ºå½“å‰é…ç½®ä¿¡æ¯"""
        print("\nğŸ“Š å½“å‰é…ç½®ä¿¡æ¯:")
        print(f"   æ¨¡å‹: {self.model}")
        print(f"   API Base: {os.getenv('OPENAI_API_BASE', 'é»˜è®¤')}")
        print(f"   å¯¹è¯è½®æ•°: {len(messages) // 2}")
        print(f"   æ€»æ¶ˆæ¯æ•°: {len(messages)}")


def single_completion_mode(client: AgenticXChatClient):
    """å•æ¬¡è¡¥å…¨æ¨¡å¼"""
    print(f"\nğŸ” å•æ¬¡è¡¥å…¨æ¨¡å¼ (æ¨¡å‹: {client.model})")
    print("ğŸ’¡ è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ŒæŒ‰å›è½¦è·å–å›å¤")
    print("=" * 50)
    
    while True:
        try:
            prompt = input("\nâ“ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ (è¾“å…¥ 'quit' é€€å‡º): ").strip()
            
            if prompt.lower() == 'quit':
                print("ğŸ‘‹ å†è§ï¼")
                break
            elif not prompt:
                print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆå†…å®¹")
                continue
            
            print("\nğŸ¤– æ­£åœ¨æ€è€ƒ...")
            response = client.completion(prompt)
            
            if response:
                print(f"\nğŸ’¬ å›å¤:\n{response}")
            else:
                print("âŒ æœªèƒ½è·å–å›å¤")
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="AgenticX LLM èŠå¤©ç¤ºä¾‹",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python llm_chat_example.py                          # é»˜è®¤äº¤äº’å¼å¯¹è¯ï¼Œä½¿ç”¨ gpt-4o-mini
  python llm_chat_example.py -m gpt-4                 # ä½¿ç”¨ gpt-4 è¿›è¡Œäº¤äº’å¼å¯¹è¯
  python llm_chat_example.py -t completion            # å•æ¬¡è¡¥å…¨æ¨¡å¼
        """
    )
    
    parser.add_argument(
        "-m", "--model",
        default="gpt-4o-mini",
        help="è¦ä½¿ç”¨çš„æ¨¡å‹åç§° (é»˜è®¤: gpt-4o-mini)"
    )
    
    parser.add_argument(
        "-t", "--task-type",
        choices=["chat", "completion"],
        default="chat",
        help="ä»»åŠ¡ç±»å‹: chat=äº¤äº’å¼å¯¹è¯, completion=å•æ¬¡è¡¥å…¨ (é»˜è®¤: chat)"
    )
    
    parser.add_argument(
        "--list-models",
        action="store_true",
        help="æ˜¾ç¤ºå¸¸ç”¨æ¨¡å‹åˆ—è¡¨"
    )
    
    args = parser.parse_args()
    
    # æ˜¾ç¤ºæ¨¡å‹åˆ—è¡¨
    if args.list_models:
        print("\nğŸ¤– å¸¸ç”¨æ¨¡å‹åˆ—è¡¨:")
        models = [
            "gpt-4o-mini (æ¨èï¼Œå¿«é€Ÿä¸”ç»æµ)",
            "gpt-4o (æœ€æ–° GPT-4)",
            "gpt-4-turbo (GPT-4 Turbo)",
            "gpt-3.5-turbo (ç»æµé€‰æ‹©)",
            "gemini/gemini-pro (Google Gemini Pro)",
            "ollama/llama3 (æœ¬åœ° Ollama)",
            "deepseek/deepseek-chat (DeepSeek)",
        ]
        for model in models:
            print(f"  â€¢ {model}")
        print("\nğŸ’¡ ä½¿ç”¨ -m å‚æ•°æŒ‡å®šæ¨¡å‹ï¼Œä¾‹å¦‚: -m gpt-4o")
        return
    
    print("ğŸš€ AgenticX LLM èŠå¤©ç¤ºä¾‹å¯åŠ¨ä¸­...")
    
    # åˆ›å»ºèŠå¤©å®¢æˆ·ç«¯ï¼ˆç¯å¢ƒå˜é‡æ£€æŸ¥åœ¨ _setup_provider ä¸­è¿›è¡Œï¼‰
    try:
        client = AgenticXChatClient(model=args.model)
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        sys.exit(1)
    
    # æ ¹æ®ä»»åŠ¡ç±»å‹æ‰§è¡Œç›¸åº”æ¨¡å¼
    if args.task_type == "chat":
        client.interactive_chat()
    else:
        single_completion_mode(client)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
