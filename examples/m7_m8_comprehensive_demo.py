"""
AgenticX M7 & M8 ç»¼åˆæ¼”ç¤ºï¼šæ™ºèƒ½æ•°æ®å¤„ç†å·¥ä½œæµ

è¿™ä¸ªæ¼”ç¤ºå±•ç¤ºäº† M7 ä»»åŠ¡å¥‘çº¦éªŒè¯å’Œ M8 å·¥ä½œæµç¼–æ’çš„å®Œæ•´åŠŸèƒ½ï¼š
- ä»»åŠ¡è¾“å‡ºè§£æå’ŒéªŒè¯
- è¾“å‡ºè‡ªä¿®å¤æœºåˆ¶
- å¤æ‚å·¥ä½œæµç¼–æ’
- æ¡ä»¶è·¯ç”±å’Œå¹¶è¡Œæ‰§è¡Œ
- äº‹ä»¶é©±åŠ¨çš„å·¥ä½œæµç®¡ç†

æ¼”ç¤ºåœºæ™¯ï¼šæ„å»ºä¸€ä¸ªæ™ºèƒ½æ•°æ®å¤„ç†å·¥ä½œæµï¼ŒåŒ…å«æ•°æ®æ”¶é›†ã€æ¸…æ´—ã€åˆ†æã€éªŒè¯å’ŒæŠ¥å‘Šç”Ÿæˆ
"""

import sys
import os
import json
import asyncio
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from agenticx.core import (
    Agent, Task, tool, WorkflowEngine, WorkflowGraph,
    TaskOutputParser, TaskResultValidator, OutputRepairLoop,
    ScheduledTrigger, EventDrivenTrigger, TriggerService,
    RepairStrategy, WorkflowStatus
)
from agenticx.llms.base import BaseLLMProvider
from agenticx.llms.response import LLMResponse, TokenUsage


# å®šä¹‰æ•°æ®æ¨¡å‹
class DataSource(BaseModel):
    """æ•°æ®æºæ¨¡å‹"""
    name: str = Field(..., description="æ•°æ®æºåç§°")
    type: str = Field(..., description="æ•°æ®æºç±»å‹")
    url: str = Field(..., description="æ•°æ®æºURL")
    status: str = Field(default="active", description="çŠ¶æ€")


class DataQuality(BaseModel):
    """æ•°æ®è´¨é‡æŠ¥å‘Šæ¨¡å‹"""
    completeness: float = Field(..., ge=0.0, le=100.0, description="å®Œæ•´æ€§ç™¾åˆ†æ¯”")
    accuracy: float = Field(..., ge=0.0, le=100.0, description="å‡†ç¡®æ€§ç™¾åˆ†æ¯”")
    consistency: float = Field(..., ge=0.0, le=100.0, description="ä¸€è‡´æ€§ç™¾åˆ†æ¯”")
    issues: List[str] = Field(default_factory=list, description="å‘ç°çš„é—®é¢˜")
    recommendations: List[str] = Field(default_factory=list, description="æ”¹è¿›å»ºè®®")


class AnalysisResult(BaseModel):
    """åˆ†æç»“æœæ¨¡å‹"""
    summary: str = Field(..., description="åˆ†ææ‘˜è¦")
    key_findings: List[str] = Field(default_factory=list, description="å…³é”®å‘ç°")
    metrics: Dict[str, float] = Field(default_factory=dict, description="å…³é”®æŒ‡æ ‡")
    trend: str = Field(..., description="è¶‹åŠ¿åˆ†æ")
    confidence: float = Field(..., ge=0.0, le=1.0, description="ç½®ä¿¡åº¦")


class ProcessingReport(BaseModel):
    """å¤„ç†æŠ¥å‘Šæ¨¡å‹"""
    workflow_id: str = Field(..., description="å·¥ä½œæµID")
    execution_time: float = Field(..., description="æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰")
    data_sources: List[DataSource] = Field(default_factory=list, description="æ•°æ®æºåˆ—è¡¨")
    quality_report: Optional[DataQuality] = Field(None, description="è´¨é‡æŠ¥å‘Š")
    analysis_result: Optional[AnalysisResult] = Field(None, description="åˆ†æç»“æœ")
    status: str = Field(..., description="å¤„ç†çŠ¶æ€")
    errors: List[str] = Field(default_factory=list, description="é”™è¯¯åˆ—è¡¨")


class MockLLMProvider(BaseLLMProvider):
    """æ¨¡æ‹Ÿ LLM æä¾›è€…ï¼Œç”¨äºæ¼”ç¤º"""
    
    def __init__(self, responses=None, name="DataProcessor"):
        super().__init__(model=f"mock-{name}")
        object.__setattr__(self, 'responses', responses or [])
        object.__setattr__(self, 'call_count', 0)
        object.__setattr__(self, 'name', name)
    
    def invoke(self, prompt: str, **kwargs) -> LLMResponse:
        if self.call_count < len(self.responses):
            content = self.responses[self.call_count]
        else:
            content = '{"status": "completed", "result": "default response"}'
        
        object.__setattr__(self, 'call_count', self.call_count + 1)
        
        return LLMResponse(
            id=f"mock-{self.name}-{self.call_count}",
            model_name=self.model,
            created=1234567890,
            content=content,
            choices=[],
            token_usage=TokenUsage(prompt_tokens=15, completion_tokens=25, total_tokens=40),
            cost=0.002
        )
    
    async def ainvoke(self, prompt: str, **kwargs) -> LLMResponse:
        return self.invoke(prompt, **kwargs)
    
    def stream(self, prompt: str, **kwargs):
        response = self.invoke(prompt, **kwargs)
        yield response.content
    
    async def astream(self, prompt: str, **kwargs):
        response = await self.ainvoke(prompt, **kwargs)
        yield response.content


# å®šä¹‰å·¥ä½œæµå·¥å…·
@tool()
def collect_data_sources() -> str:
    """æ”¶é›†æ•°æ®æºä¿¡æ¯"""
    sources = [
        {
            "name": "ç”¨æˆ·è¡Œä¸ºæ•°æ®åº“",
            "type": "database",
            "url": "postgresql://localhost:5432/user_behavior",
            "status": "active"
        },
        {
            "name": "å¤–éƒ¨APIæ•°æ®",
            "type": "api",
            "url": "https://api.example.com/data",
            "status": "active"
        },
        {
            "name": "æ—¥å¿—æ–‡ä»¶",
            "type": "file",
            "url": "/var/log/application.log",
            "status": "active"
        }
    ]
    
    return json.dumps(sources, ensure_ascii=False)


@tool()
def validate_data_sources(sources_json: str) -> str:
    """éªŒè¯æ•°æ®æºçš„å¯ç”¨æ€§"""
    try:
        sources = json.loads(sources_json)
        validated_sources = []
        
        for source in sources:
            # æ¨¡æ‹ŸéªŒè¯é€»è¾‘
            if source.get("status") == "active":
                source["validation_status"] = "passed"
                source["last_checked"] = datetime.now().isoformat()
            else:
                source["validation_status"] = "failed"
                source["error"] = "æ•°æ®æºä¸å¯ç”¨"
            
            validated_sources.append(source)
        
        return json.dumps(validated_sources, ensure_ascii=False)
    
    except Exception as e:
        return json.dumps({"error": f"éªŒè¯å¤±è´¥: {str(e)}"}, ensure_ascii=False)


@tool()
def clean_data(sources_json: str) -> str:
    """æ¸…æ´—æ•°æ®"""
    try:
        sources = json.loads(sources_json)
        
        # æ¨¡æ‹Ÿæ•°æ®æ¸…æ´—è¿‡ç¨‹
        cleaning_results = {
            "total_records": 10000,
            "cleaned_records": 9500,
            "removed_duplicates": 300,
            "fixed_missing_values": 200,
            "cleaning_rules_applied": [
                "ç§»é™¤é‡å¤è®°å½•",
                "å¡«å……ç¼ºå¤±å€¼",
                "æ ‡å‡†åŒ–æ ¼å¼",
                "éªŒè¯æ•°æ®ç±»å‹"
            ]
        }
        
        return json.dumps(cleaning_results, ensure_ascii=False)
    
    except Exception as e:
        return json.dumps({"error": f"æ•°æ®æ¸…æ´—å¤±è´¥: {str(e)}"}, ensure_ascii=False)


@tool()
def analyze_data_quality(cleaning_results_json: str) -> str:
    """åˆ†ææ•°æ®è´¨é‡"""
    try:
        # æ¨¡æ‹Ÿæ•°æ®è´¨é‡åˆ†æ
        quality_report = {
            "completeness": 95.0,
            "accuracy": 92.5,
            "consistency": 88.0,
            "issues": [
                "éƒ¨åˆ†è®°å½•ç¼ºå°‘æ—¶é—´æˆ³",
                "ç”¨æˆ·IDæ ¼å¼ä¸ä¸€è‡´",
                "æ•°å€¼å­—æ®µå­˜åœ¨å¼‚å¸¸å€¼"
            ],
            "recommendations": [
                "å»ºç«‹æ•°æ®éªŒè¯è§„åˆ™",
                "ç»Ÿä¸€ç”¨æˆ·IDæ ¼å¼",
                "è®¾ç½®æ•°å€¼èŒƒå›´æ£€æŸ¥"
            ]
        }
        
        return json.dumps(quality_report, ensure_ascii=False)
    
    except Exception as e:
        return json.dumps({"error": f"è´¨é‡åˆ†æå¤±è´¥: {str(e)}"}, ensure_ascii=False)


@tool()
def perform_analysis(quality_report_json: str) -> str:
    """æ‰§è¡Œæ•°æ®åˆ†æ"""
    try:
        # æ¨¡æ‹Ÿå¤æ‚çš„æ•°æ®åˆ†æ
        analysis_result = {
            "summary": "ç”¨æˆ·è¡Œä¸ºæ•°æ®åˆ†ææ˜¾ç¤ºæ´»è·ƒåº¦å‘ˆä¸Šå‡è¶‹åŠ¿",
            "key_findings": [
                "ç§»åŠ¨ç«¯ç”¨æˆ·å¢é•¿30%",
                "å¹³å‡ä¼šè¯æ—¶é•¿å¢åŠ 15%",
                "è½¬åŒ–ç‡æå‡8%",
                "ç”¨æˆ·ç•™å­˜ç‡è¾¾åˆ°75%"
            ],
            "metrics": {
                "daily_active_users": 15000.0,
                "session_duration": 8.5,
                "conversion_rate": 12.3,
                "retention_rate": 75.0
            },
            "trend": "positive",
            "confidence": 0.89
        }
        
        return json.dumps(analysis_result, ensure_ascii=False)
    
    except Exception as e:
        return json.dumps({"error": f"åˆ†æå¤±è´¥: {str(e)}"}, ensure_ascii=False)


@tool()
def generate_report(analysis_json: str, quality_json: str, workflow_id: str) -> str:
    """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
    try:
        analysis = json.loads(analysis_json)
        quality = json.loads(quality_json)
        
        # åˆ›å»ºç»¼åˆæŠ¥å‘Š
        report = {
            "workflow_id": workflow_id,
            "execution_time": 45.6,
            "data_sources": [
                {
                    "name": "ç”¨æˆ·è¡Œä¸ºæ•°æ®åº“",
                    "type": "database",
                    "url": "postgresql://localhost:5432/user_behavior",
                    "status": "active"
                }
            ],
            "quality_report": quality,
            "analysis_result": analysis,
            "status": "completed",
            "errors": []
        }
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_filename = f"data_processing_report_{workflow_id}.json"
        with open(report_filename, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        return json.dumps({
            "report_file": report_filename,
            "summary": "æ•°æ®å¤„ç†å·¥ä½œæµæ‰§è¡Œå®Œæˆ",
            "status": "success"
        }, ensure_ascii=False)
    
    except Exception as e:
        return json.dumps({"error": f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"}, ensure_ascii=False)


@tool()
def send_notification(report_json: str) -> str:
    """å‘é€é€šçŸ¥"""
    try:
        report_data = json.loads(report_json)
        
        # æ¨¡æ‹Ÿå‘é€é€šçŸ¥
        notification = {
            "type": "email",
            "recipients": ["admin@company.com", "data-team@company.com"],
            "subject": "æ•°æ®å¤„ç†å·¥ä½œæµå®Œæˆ",
            "message": f"å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼ŒæŠ¥å‘Šæ–‡ä»¶ï¼š{report_data.get('report_file', 'unknown')}",
            "sent_at": datetime.now().isoformat(),
            "status": "sent"
        }
        
        return json.dumps(notification, ensure_ascii=False)
    
    except Exception as e:
        return json.dumps({"error": f"é€šçŸ¥å‘é€å¤±è´¥: {str(e)}"}, ensure_ascii=False)


class DataProcessingWorkflow:
    """æ•°æ®å¤„ç†å·¥ä½œæµç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–å·¥ä½œæµ"""
        self.engine = WorkflowEngine(max_concurrent_nodes=5)
        self.parser = TaskOutputParser(enable_fuzzy_parsing=True)
        self.validator = TaskResultValidator()
        self.repair_loop = OutputRepairLoop(
            max_repair_attempts=2,
            repair_strategy=RepairStrategy.SIMPLE
        )
        
        # åˆ›å»ºè§¦å‘å™¨æœåŠ¡
        self.trigger_service = TriggerService()
        self._setup_triggers()
    
    def _setup_triggers(self):
        """è®¾ç½®è§¦å‘å™¨"""
        # å®šæ—¶è§¦å‘å™¨ï¼šæ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡
        scheduled_trigger = ScheduledTrigger(
            "data_processing_workflow",
            "hourly",
            {"trigger_type": "scheduled", "source": "cron"}
        )
        
        # äº‹ä»¶è§¦å‘å™¨ï¼šæ•°æ®æ›´æ–°æ—¶è§¦å‘
        event_trigger = EventDrivenTrigger(
            "data_processing_workflow",
            "data_updated"
        )
        
        self.trigger_service.register_trigger("hourly_processing", scheduled_trigger)
        self.trigger_service.register_trigger("data_update_processing", event_trigger)
    
    def create_workflow_graph(self) -> WorkflowGraph:
        """åˆ›å»ºå·¥ä½œæµå›¾"""
        graph = WorkflowGraph()
        
        # æ·»åŠ èŠ‚ç‚¹
        graph.add_node("collect_sources", collect_data_sources, "tool", {
            "description": "æ”¶é›†æ‰€æœ‰å¯ç”¨çš„æ•°æ®æº"
        })
        
        graph.add_node("validate_sources", validate_data_sources, "tool", {
            "description": "éªŒè¯æ•°æ®æºçš„å¯ç”¨æ€§",
            "args": {"sources_json": "${collect_sources}"}
        })
        
        graph.add_node("clean_data", clean_data, "tool", {
            "description": "æ¸…æ´—å’Œé¢„å¤„ç†æ•°æ®",
            "args": {"sources_json": "${validate_sources}"}
        })
        
        # å¹¶è¡Œæ‰§è¡Œè´¨é‡åˆ†æå’Œæ•°æ®åˆ†æ
        graph.add_node("quality_analysis", analyze_data_quality, "tool", {
            "description": "åˆ†ææ•°æ®è´¨é‡",
            "args": {"cleaning_results_json": "${clean_data}"}
        })
        
        graph.add_node("data_analysis", perform_analysis, "tool", {
            "description": "æ‰§è¡Œæ•°æ®åˆ†æ",
            "args": {"quality_report_json": "${quality_analysis}"}
        })
        
        # ç”ŸæˆæŠ¥å‘Š
        graph.add_node("generate_report", generate_report, "tool", {
            "description": "ç”Ÿæˆç»¼åˆæŠ¥å‘Š",
            "args": {
                "analysis_json": "${data_analysis}",
                "quality_json": "${quality_analysis}",
                "workflow_id": "${workflow_execution_id}"
            }
        })
        
        # å‘é€é€šçŸ¥
        graph.add_node("send_notification", send_notification, "tool", {
            "description": "å‘é€å®Œæˆé€šçŸ¥",
            "args": {"report_json": "${generate_report}"}
        })
        
        # æ·»åŠ è¾¹ï¼ˆå®šä¹‰æ‰§è¡Œé¡ºåºï¼‰
        graph.add_edge("collect_sources", "validate_sources")
        graph.add_edge("validate_sources", "clean_data")
        graph.add_edge("clean_data", "quality_analysis")
        graph.add_edge("quality_analysis", "data_analysis")
        graph.add_edge("data_analysis", "generate_report")
        graph.add_edge("generate_report", "send_notification")
        
        return graph
    
    def create_conditional_workflow_graph(self) -> WorkflowGraph:
        """åˆ›å»ºå¸¦æ¡ä»¶è·¯ç”±çš„å·¥ä½œæµå›¾"""
        graph = WorkflowGraph()
        
        # æ·»åŠ åŸºç¡€èŠ‚ç‚¹
        graph.add_node("collect_sources", collect_data_sources, "tool", {
            "description": "æ”¶é›†æ•°æ®æº"
        })
        
        graph.add_node("validate_sources", validate_data_sources, "tool", {
            "description": "éªŒè¯æ•°æ®æº",
            "args": {"sources_json": "${collect_sources}"}
        })
        
        # æ·»åŠ æ¡ä»¶åˆ†æ”¯èŠ‚ç‚¹
        graph.add_node("clean_data_basic", clean_data, "tool", {
            "mode": "basic",
            "description": "åŸºç¡€æ•°æ®æ¸…æ´—",
            "args": {"sources_json": "${validate_sources}"}
        })
        
        graph.add_node("clean_data_advanced", clean_data, "tool", {
            "mode": "advanced", 
            "description": "é«˜çº§æ•°æ®æ¸…æ´—",
            "args": {"sources_json": "${validate_sources}"}
        })
        
        # æ·»åŠ åˆ†æèŠ‚ç‚¹
        graph.add_node("quality_analysis", analyze_data_quality, "tool", {
            "description": "åˆ†ææ•°æ®è´¨é‡",
            "args": {"cleaning_results_json": "${clean_data_basic}${clean_data_advanced}"}
        })
        
        graph.add_node("generate_report", generate_report, "tool", {
            "description": "ç”ŸæˆæŠ¥å‘Š",
            "args": {
                "analysis_json": "{}",  # ç®€åŒ–çš„åˆ†æç»“æœ
                "quality_json": "${quality_analysis}",
                "workflow_id": "${workflow_execution_id}"
            }
        })
        
        # æ·»åŠ è¾¹å’Œæ¡ä»¶
        graph.add_edge("collect_sources", "validate_sources")
        
        # æ¡ä»¶è·¯ç”±ï¼šæ ¹æ®æ•°æ®æºæ•°é‡é€‰æ‹©æ¸…æ´—ç­–ç•¥
        graph.add_edge(
            "validate_sources", 
            "clean_data_basic",
            lambda result: self._count_data_sources(result) <= 2
        )
        
        graph.add_edge(
            "validate_sources",
            "clean_data_advanced", 
            lambda result: self._count_data_sources(result) > 2
        )
        
        # æ±‡èšåˆ°è´¨é‡åˆ†æ
        graph.add_edge("clean_data_basic", "quality_analysis")
        graph.add_edge("clean_data_advanced", "quality_analysis")
        graph.add_edge("quality_analysis", "generate_report")
        
        return graph
    
    def _count_data_sources(self, result: str) -> int:
        """è®¡ç®—æ•°æ®æºæ•°é‡"""
        try:
            data = json.loads(result)
            if isinstance(data, list):
                return len(data)
            elif isinstance(data, dict) and "sources" in data:
                return len(data["sources"])
            else:
                return 1
        except:
            return 1
    
    async def run_basic_workflow(self) -> Dict[str, Any]:
        """è¿è¡ŒåŸºç¡€å·¥ä½œæµ"""
        print("ğŸš€ å¼€å§‹æ‰§è¡ŒåŸºç¡€æ•°æ®å¤„ç†å·¥ä½œæµ")
        print("=" * 60)
        
        # åˆ›å»ºå·¥ä½œæµå›¾
        graph = self.create_workflow_graph()
        
        # æ‰§è¡Œå·¥ä½œæµ
        start_time = datetime.now()
        context = await self.engine.run(
            graph, 
            {"workflow_execution_id": f"basic_{int(start_time.timestamp())}"}
        )
        end_time = datetime.now()
        
        # æ˜¾ç¤ºæ‰§è¡Œç»“æœ
        print(f"\nğŸ“Š å·¥ä½œæµæ‰§è¡Œç»“æœ:")
        print(f"  çŠ¶æ€: {'âœ… æˆåŠŸ' if context.status == WorkflowStatus.COMPLETED else 'âŒ å¤±è´¥'}")
        print(f"  æ‰§è¡Œæ—¶é—´: {(end_time - start_time).total_seconds():.2f} ç§’")
        print(f"  æ‰§è¡Œçš„èŠ‚ç‚¹æ•°: {len(context.node_results)}")
        
        # æ˜¾ç¤ºæ¯ä¸ªèŠ‚ç‚¹çš„ç»“æœ
        print(f"\nğŸ“‹ èŠ‚ç‚¹æ‰§è¡Œè¯¦æƒ…:")
        for node_id, result in context.node_results.items():
            print(f"  â€¢ {node_id}: {str(result)[:100]}...")
        
        return {
            "status": context.status.value,
            "execution_time": (end_time - start_time).total_seconds(),
            "node_results": context.node_results,
            "event_count": len(context.event_log.events)
        }
    
    async def run_conditional_workflow(self) -> Dict[str, Any]:
        """è¿è¡Œæ¡ä»¶å·¥ä½œæµ"""
        print("ğŸ”€ å¼€å§‹æ‰§è¡Œæ¡ä»¶è·¯ç”±å·¥ä½œæµ")
        print("=" * 60)
        
        # åˆ›å»ºæ¡ä»¶å·¥ä½œæµå›¾
        graph = self.create_conditional_workflow_graph()
        
        # æ‰§è¡Œå·¥ä½œæµ
        start_time = datetime.now()
        context = await self.engine.run(
            graph,
            {"workflow_execution_id": f"conditional_{int(start_time.timestamp())}"}
        )
        end_time = datetime.now()
        
        # æ˜¾ç¤ºæ‰§è¡Œç»“æœ
        print(f"\nğŸ“Š æ¡ä»¶å·¥ä½œæµæ‰§è¡Œç»“æœ:")
        print(f"  çŠ¶æ€: {'âœ… æˆåŠŸ' if context.status == WorkflowStatus.COMPLETED else 'âŒ å¤±è´¥'}")
        print(f"  æ‰§è¡Œæ—¶é—´: {(end_time - start_time).total_seconds():.2f} ç§’")
        print(f"  æ‰§è¡Œçš„èŠ‚ç‚¹æ•°: {len(context.node_results)}")
        
        # åˆ†ææ‰§è¡Œè·¯å¾„
        executed_nodes = list(context.node_results.keys())
        if "clean_data_basic" in executed_nodes:
            print("  ğŸ›¤ï¸  æ‰§è¡Œè·¯å¾„: åŸºç¡€æ¸…æ´—è·¯å¾„")
        elif "clean_data_advanced" in executed_nodes:
            print("  ğŸ›¤ï¸  æ‰§è¡Œè·¯å¾„: é«˜çº§æ¸…æ´—è·¯å¾„")
        
        return {
            "status": context.status.value,
            "execution_time": (end_time - start_time).total_seconds(),
            "executed_path": "basic" if "clean_data_basic" in executed_nodes else "advanced",
            "node_results": context.node_results
        }
    
    def demonstrate_task_validation(self) -> Dict[str, Any]:
        """æ¼”ç¤ºä»»åŠ¡éªŒè¯åŠŸèƒ½"""
        print("ğŸ” æ¼”ç¤ºä»»åŠ¡è¾“å‡ºè§£æå’ŒéªŒè¯")
        print("=" * 60)
        
        # æµ‹è¯•ç”¨ä¾‹1ï¼šæ­£ç¡®çš„JSONè¾“å‡º
        print("\nğŸ“ æµ‹è¯•ç”¨ä¾‹1ï¼šæ­£ç¡®çš„JSONè¾“å‡º")
        correct_response = '''
        {
            "completeness": 95.0,
            "accuracy": 92.5,
            "consistency": 88.0,
            "issues": ["æ•°æ®ç¼ºå¤±", "æ ¼å¼ä¸ä¸€è‡´"],
            "recommendations": ["å¢åŠ éªŒè¯", "ç»Ÿä¸€æ ¼å¼"]
        }
        '''
        
        parse_result = self.parser.parse(correct_response, DataQuality)
        print(f"  è§£æç»“æœ: {'âœ… æˆåŠŸ' if parse_result.success else 'âŒ å¤±è´¥'}")
        if parse_result.success:
            validation_result = self.validator.validate(parse_result.data)
            print(f"  éªŒè¯ç»“æœ: {'âœ… é€šè¿‡' if validation_result.valid else 'âŒ å¤±è´¥'}")
        
        # æµ‹è¯•ç”¨ä¾‹2ï¼šæ ¼å¼é”™è¯¯çš„è¾“å‡ºï¼ˆéœ€è¦ä¿®å¤ï¼‰
        print("\nğŸ“ æµ‹è¯•ç”¨ä¾‹2ï¼šæ ¼å¼é”™è¯¯çš„è¾“å‡ºï¼ˆå•å¼•å·ï¼‰")
        malformed_response = '''
        {
            'completeness': 95.0,
            'accuracy': 92.5,
            'consistency': 88.0,
            'issues': ['æ•°æ®ç¼ºå¤±', 'æ ¼å¼ä¸ä¸€è‡´'],
            'recommendations': ['å¢åŠ éªŒè¯', 'ç»Ÿä¸€æ ¼å¼']
        }
        '''
        
        parse_result = self.parser.parse(malformed_response, DataQuality)
        print(f"  åˆå§‹è§£æ: {'âœ… æˆåŠŸ' if parse_result.success else 'âŒ å¤±è´¥'}")
        
        if not parse_result.success:
            print("  ğŸ”§ å°è¯•è‡ªåŠ¨ä¿®å¤...")
            repaired_result = self.repair_loop.repair(
                malformed_response, parse_result, None, DataQuality
            )
            print(f"  ä¿®å¤ç»“æœ: {'âœ… æˆåŠŸ' if repaired_result.success else 'âŒ å¤±è´¥'}")
        
        # æµ‹è¯•ç”¨ä¾‹3ï¼šä»Markdownæå–JSON
        print("\nğŸ“ æµ‹è¯•ç”¨ä¾‹3ï¼šä»Markdownä»£ç å—æå–JSON")
        markdown_response = '''
        æ ¹æ®æ•°æ®åˆ†æï¼Œæˆ‘å¾—å‡ºä»¥ä¸‹è´¨é‡è¯„ä¼°ç»“æœï¼š
        
        ```json
        {
            "completeness": 98.5,
            "accuracy": 94.0,
            "consistency": 91.5,
            "issues": ["å°‘é‡é‡å¤æ•°æ®"],
            "recommendations": ["å»é‡å¤„ç†"]
        }
        ```
        
        è¿™æ˜¯è¯¦ç»†çš„è´¨é‡åˆ†ææŠ¥å‘Šã€‚
        '''
        
        parse_result = self.parser.parse(markdown_response, DataQuality)
        print(f"  è§£æç»“æœ: {'âœ… æˆåŠŸ' if parse_result.success else 'âŒ å¤±è´¥'}")
        print(f"  ç½®ä¿¡åº¦: {parse_result.confidence:.2f}")
        
        return {
            "test_cases": 3,
            "successful_parses": sum([
                1 if self.parser.parse(resp, DataQuality).success else 0
                for resp in [correct_response, malformed_response, markdown_response]
            ])
        }
    
    def start_trigger_service(self):
        """å¯åŠ¨è§¦å‘å™¨æœåŠ¡"""
        print("â° å¯åŠ¨è§¦å‘å™¨æœåŠ¡")
        self.trigger_service.start()
        print("  âœ… å®šæ—¶è§¦å‘å™¨å·²å¯åŠ¨ï¼ˆæ¯å°æ—¶æ‰§è¡Œï¼‰")
        print("  âœ… äº‹ä»¶è§¦å‘å™¨å·²å¯åŠ¨ï¼ˆç›‘å¬æ•°æ®æ›´æ–°äº‹ä»¶ï¼‰")
    
    def stop_trigger_service(self):
        """åœæ­¢è§¦å‘å™¨æœåŠ¡"""
        print("â¹ï¸  åœæ­¢è§¦å‘å™¨æœåŠ¡")
        self.trigger_service.stop()


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ AgenticX M7 & M8 ç»¼åˆæ¼”ç¤º")
    print("æ•°æ®å¤„ç†å·¥ä½œæµ - å±•ç¤ºä»»åŠ¡éªŒè¯å’Œå·¥ä½œæµç¼–æ’")
    print("=" * 80)
    
    # åˆ›å»ºå·¥ä½œæµå®ä¾‹
    workflow = DataProcessingWorkflow()
    
    try:
        # 1. æ¼”ç¤ºä»»åŠ¡éªŒè¯åŠŸèƒ½
        print("\n" + "="*80)
        print("ğŸ” ç¬¬ä¸€éƒ¨åˆ†ï¼šä»»åŠ¡è¾“å‡ºè§£æå’ŒéªŒè¯æ¼”ç¤º")
        print("="*80)
        
        validation_results = workflow.demonstrate_task_validation()
        print(f"\nğŸ“Š éªŒè¯æ¼”ç¤ºæ€»ç»“:")
        print(f"  æµ‹è¯•ç”¨ä¾‹æ•°: {validation_results['test_cases']}")
        print(f"  æˆåŠŸè§£ææ•°: {validation_results['successful_parses']}")
        
        # 2. æ¼”ç¤ºåŸºç¡€å·¥ä½œæµ
        print("\n" + "="*80)
        print("ğŸš€ ç¬¬äºŒéƒ¨åˆ†ï¼šåŸºç¡€å·¥ä½œæµç¼–æ’æ¼”ç¤º")
        print("="*80)
        
        basic_results = await workflow.run_basic_workflow()
        print(f"\nğŸ“ˆ åŸºç¡€å·¥ä½œæµç»Ÿè®¡:")
        print(f"  æ‰§è¡ŒçŠ¶æ€: {basic_results['status']}")
        print(f"  æ‰§è¡Œæ—¶é—´: {basic_results['execution_time']:.2f} ç§’")
        print(f"  äº‹ä»¶æ•°é‡: {basic_results['event_count']}")
        
        # 3. æ¼”ç¤ºæ¡ä»¶å·¥ä½œæµ
        print("\n" + "="*80)
        print("ğŸ”€ ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ¡ä»¶è·¯ç”±å·¥ä½œæµæ¼”ç¤º")
        print("="*80)
        
        conditional_results = await workflow.run_conditional_workflow()
        print(f"\nğŸ“ˆ æ¡ä»¶å·¥ä½œæµç»Ÿè®¡:")
        print(f"  æ‰§è¡ŒçŠ¶æ€: {conditional_results['status']}")
        print(f"  æ‰§è¡Œè·¯å¾„: {conditional_results['executed_path']}")
        print(f"  æ‰§è¡Œæ—¶é—´: {conditional_results['execution_time']:.2f} ç§’")
        
        # 4. æ¼”ç¤ºè§¦å‘å™¨æœåŠ¡
        print("\n" + "="*80)
        print("â° ç¬¬å››éƒ¨åˆ†ï¼šè§¦å‘å™¨æœåŠ¡æ¼”ç¤º")
        print("="*80)
        
        workflow.start_trigger_service()
        
        # æ¨¡æ‹Ÿè¿è¡Œä¸€æ®µæ—¶é—´
        print("  â³ æ¨¡æ‹Ÿè¿è¡Œ 3 ç§’...")
        await asyncio.sleep(3)
        
        workflow.stop_trigger_service()
        
        # 5. æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶
        print("\n" + "="*80)
        print("ğŸ“„ ç”Ÿæˆçš„æ–‡ä»¶")
        print("="*80)
        
        import glob
        report_files = glob.glob("data_processing_report_*.json")
        if report_files:
            for file in report_files:
                print(f"  âœ… {file}")
                # æ˜¾ç¤ºæ–‡ä»¶å†…å®¹æ‘˜è¦
                try:
                    with open(file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        print(f"     å·¥ä½œæµID: {data.get('workflow_id', 'N/A')}")
                        print(f"     çŠ¶æ€: {data.get('status', 'N/A')}")
                        print(f"     æ‰§è¡Œæ—¶é—´: {data.get('execution_time', 'N/A')} ç§’")
                except Exception as e:
                    print(f"     è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        else:
            print("  âš ï¸  æœªæ‰¾åˆ°ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶")
        
        print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ“‹ æ¼”ç¤ºæ€»ç»“:")
        print("  âœ… M7 ä»»åŠ¡éªŒè¯ï¼šæ”¯æŒJSONè§£æã€æ ¼å¼ä¿®å¤ã€SchemaéªŒè¯")
        print("  âœ… M8 å·¥ä½œæµç¼–æ’ï¼šæ”¯æŒé¡ºåºæ‰§è¡Œã€å¹¶è¡Œå¤„ç†ã€æ¡ä»¶è·¯ç”±")
        print("  âœ… è§¦å‘å™¨æœåŠ¡ï¼šæ”¯æŒå®šæ—¶è§¦å‘å’Œäº‹ä»¶é©±åŠ¨")
        print("  âœ… é”™è¯¯å¤„ç†ï¼šè‡ªåŠ¨ä¿®å¤å’Œä¼˜é›…é™çº§")
        print("  âœ… å¯è§‚æµ‹æ€§ï¼šå®Œæ•´çš„äº‹ä»¶æ—¥å¿—å’Œæ‰§è¡Œç»Ÿè®¡")
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ç¡®ä¿æ¸…ç†èµ„æº
        workflow.stop_trigger_service()


if __name__ == "__main__":
    asyncio.run(main()) 